{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48d0c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's coding time again!\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-3/programming_skills.png\" width=\"1800\" alt='programming_skills'/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fad4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's recall your interests\n",
    "\n",
    "61.8% is interested in programming mistakes!\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-1/google_form_true.png\" width=\"2200\" alt='google_form_true'/>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e770bcf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's focus on this topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a981a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need this lecture?\n",
    "\n",
    "Whether you like it or not, experimental setting might require you to do some coding stuff.\n",
    "\n",
    "Coding translates to: \n",
    "\n",
    "1. Transparency (*don't you dare do some cheap tricks!*)\n",
    "2. Correctness (*your code should reflect your paper statements*) \n",
    "3. **Readability** (*please, don't make this a nightmare*)\n",
    "4. Efficiency (*time is money*)\n",
    "5. **Maintainability** (*I'm sure you'll re-use this code*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda74f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the past lecture, we have shown some tools to make our code more efficient in Tensorflow and Pytorch [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc969f07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now provide some tips & tricks concerning [3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee19f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we going to cover?\n",
    "\n",
    "- Debugging\n",
    "- General coding best practices\n",
    "- Tensorflow and PyTorch best practices\n",
    "- Misc: code documentation, README, controlled environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9afc8f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*The 5-minute-in-the-future of yourself and your friends will appreciate!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b02b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Debugging\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-4/programming_meme.png\" width=\"600\"/>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb46f01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What are we going to see\n",
    "\n",
    "- Using a debugger\n",
    "- Type hints\n",
    "- Typechecking\n",
    "- Assertions\n",
    "- Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aaebbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The debugging slice\n",
    "\n",
    "Whether you've already realized it or you still have to, debugging usually takes around 90% of your work time\n",
    "\n",
    "- 9% the idea\n",
    "- 1% code writing\n",
    "- 90% debugging\n",
    "\n",
    "It is tiresome, boring, stressful, annoying $\\rightarrow$ we all know that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441e29a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### My point\n",
    "\n",
    "- We don't really need to be good programmers to write correct code (*there are many high-level libraries!*)\n",
    "- This lecture is not a computer science 101 course about programming (*I'm far from being competent on this matter*)\n",
    "- What I want to say is that you should learn how to think to tackle debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6484209",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Debugging is like an investigation game where you have to find the culprit, and usually the culprit is you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e2a21",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What are our weapons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce8be4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Dynamic] Debugger\n",
    "- [Static] Type hints\n",
    "- [Dynamic] Typechecking\n",
    "- [Dynamic] Assertions\n",
    "- [Dynamic] Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e72a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All these methods, combined, allow us to better inspect our code to find unwanted behaviours/features (*bugs*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2fbde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using a Debugger\n",
    "\n",
    "Long story short, we have a powerful tool to inspect our code\n",
    "\n",
    "- Stop using ```print(...)``` for debugging\n",
    "- Just use a debugger\n",
    "- Just use a debugger\n",
    "- Just use a debugger\n",
    "- Got it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7553a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are many powerful IDEs that support a debugger: PyCharm, Spider, VisualStudio Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566a2e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Unless you are a skilled programmer, just avoid programming with \n",
    "   - text editors like vim, sublimetext, nano, notepad++, etc.\n",
    "   - notebooks (unless you need to run real experiments)\n",
    "\n",
    "*personal opinion*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ffbb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Type hints [[Read1](https://peps.python.org/pep-0483/), [Read2](https://bernat.tech/posts/the-state-of-type-hints-in-python/)]\n",
    "\n",
    "If debugging is our dynamic way of inspecting code, type hints are our way to statically analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad40c22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Typle, List\n",
    "\n",
    "# With type hints (torch_datapipe_record_pipeline.py, Lines 46-53)\n",
    "def parse_inputs(input_data: Tuple[str, int]) -> [List[int], int]:\n",
    "    text, label = input_data\n",
    "    text = preprocess_text(text=text)\n",
    "    tokens = vocab(tokenizer(text))\n",
    "    return tokens, label\n",
    "\n",
    "\n",
    "# Without type hints\n",
    "def parse_inputs(input_data):\n",
    "    text, label = input_data\n",
    "    text = preprocess_text(text=text)\n",
    "    tokens = vocab(tokenizer(text))\n",
    "    return tokens, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc54b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Advantages\n",
    "\n",
    "- Integrated documentation with code $\\rightarrow$ much more readable than docstrings\n",
    "- Accurate code re-factoring for IDEs\n",
    "- Allows code auto-completion for IDEs\n",
    "- Linters (included in IDEs) can tell wrong function calls based on type hints $\\rightarrow$ warnings!!\n",
    "- We can define compund type like ```List[int]```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c808b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Disadvantages\n",
    "\n",
    "- We need at least ```Python 3.6``` (*reasonable given any deep learning framework requirements*)\n",
    "- May have conflicts with docstrings depending on the tool used\n",
    "- Minor added computation overhead\n",
    "- Forces to import all type dependencies, even though they are not used at runtime at all\n",
    "- Compound type may require some additional operations by the interpreter\n",
    "\n",
    "The last two points are solved via post-poned evaluation of annotations (*requires ```Python 3.7```*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10b5f6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# w/o post-poned evaluation\n",
    "class A:\n",
    "    def f(self) -> A: # NameError: name 'A' is not defined\n",
    "        pass\n",
    "    \n",
    "# w/ post-poned evaluation\n",
    "from __future__ import annotations\n",
    "\n",
    "class A:\n",
    "    def f(self) -> A:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9429b3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### An example\n",
    "\n",
    "We can use Python's reference linter ```mypy``` to run our type hinted code\n",
    "\n",
    "\n",
    "#### Script\n",
    "\n",
    "```mypy_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364adc1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Types of type hints\n",
    "\n",
    "- Nominal types: ```int```, ```float```, ```bool```, etc... (*all bultin type*)\n",
    "- Compound types: ```List[int]```\n",
    "    - We can also define type aliases for readability: ```CustomType = Optiona[List[int], Dict[str, str]]\n",
    "    \n",
    "- Compotional types: ```Union[...]``` (*one of*), ```Intersection[...]``` (*each one*), ```Optional[...]``` (*can be None*)\n",
    "- Generic types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46110c90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypeVar, Generic, Iterable, Iterator\n",
    "\n",
    "T = TypeVar('T')   # must use the same variable name\n",
    "\n",
    "class CustomClass(Generic[T]):\n",
    "    def __init__(self, value: T) -> None:\n",
    "        self.value: T = value\n",
    "            \n",
    "    def get_iterator(values: Iterable[CustomClass[int]]) -> Iterator:\n",
    "        for value in values:\n",
    "            yield value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2749b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  Proper function overloading\n",
    "\n",
    "Suppose you have the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fd145",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def do_stuff(x: Union[int, List[int]]) -> Union[int, List[int]]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d30c78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The type hinter understands that you can call ```do_stuff``` with\n",
    "   - ```input x is int returns int```\n",
    "   - ```input x is int returns List[int]```\n",
    "   - ```input x is List[int] returns int```\n",
    "   - ```input x is List[int] returns List[int]```\n",
    "   \n",
    "#### How to avoid this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244abf0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from typing import overload\n",
    "\n",
    "@overload\n",
    "def do_stuff(x: int) -> int:\n",
    "    ...\n",
    "    \n",
    "@overload\n",
    "def do_stuff(x: List[int]) -> List[int]:\n",
    "    ...\n",
    "\n",
    "\n",
    "def do_stuff(x: Union[int, List[int]]) -> Union[int, List[int]]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a1459",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What type hints don't do\n",
    "\n",
    "- Runtime type inference $\\rightarrow$ we need some libraries that leverage type hints\n",
    "- No performance tuning $\\rightarrow$ type hints are treated just like comments\n",
    "\n",
    "#### Only type hinted code is type-checked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861eb0cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bonus: type hints and Sphinx for merging documentation\n",
    "\n",
    "We can remove all typing information from docstrings and infer them from type hints\n",
    "\n",
    "Sphinx has the plugin [```agronoholm/sphinx-autodoc-typehints```](https://github.com/agronholm/sphinx-autodoc-typehints)\n",
    "\n",
    "Then add the following extensions to Sphinx's ```conf.py```\n",
    "\n",
    "```\n",
    "extensions = [\"sphinx.ext.autodoc\", \"sphinx_autodoc_typehints\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b7cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Typechecking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba46740",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1c7d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8641fbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Coding Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c87e28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What are we going to see\n",
    "\n",
    "- Naming choice\n",
    "- Comments\n",
    "- Nesting\n",
    "- Inheritance\n",
    "- Abstraction\n",
    "- Organization\n",
    "- Profiling\n",
    "- Code optimization\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f6856",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow and PyTorch Best Practices\n",
    "\n",
    "### Modeling ([Karpathy's blog](https://karpathy.github.io/2019/04/25/recipe/))\n",
    "- Inspect data\n",
    "- Start simple\n",
    "- Overfit\n",
    "- Apply some regularizations\n",
    "- Hyper-parameters tuning\n",
    "\n",
    "### Scripting\n",
    "- File Organization\n",
    "- Sequential layers\n",
    "- Calling layers\n",
    "- Training and evaluation modes\n",
    "- Mixing operations\n",
    "- Detaching\n",
    "- Numerical stability\n",
    "- Shuffling data\n",
    "- Gradient clipping\n",
    "- tf.reshape/th.view vs tf.transpose/th.permute\n",
    "- Squeezing\n",
    "- Working with GPU devices\n",
    "- Freeing GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301703c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling\n",
    "\n",
    "*Because, eventually, we all prefer a todo listing...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe98dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intro\n",
    "\n",
    "Neural networks are not a plug-and-play module that is expected to work effortlessly\n",
    "\n",
    "Instead, neural networks often fail silently (*no exceptions!*) and can still manage to reach some satisfying result (*unexpected*)\n",
    "   - Leakage\n",
    "   - Data errors\n",
    "   - Wrong initialization, regularization, optimization, etc...\n",
    "    \n",
    "#### What you should do (*in pills*)\n",
    "\n",
    "- Start slow $\\rightarrow$ don't be eager to test out your super fancy model on your ultra fancy data\n",
    "- Check your data attentively\n",
    "- Start simple $\\rightarrow$ progressively introduce complexity\n",
    "- Hammer your model until it is sufficiently powerful and regularized to work well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4bd12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Become one with the data\n",
    "\n",
    "Intuitively, we never start from modeling, but rather we look at data\n",
    "\n",
    "- Understand relevant features for addressing the task\n",
    "- Spot outliers (*may be symptomatic of data collection errors*)\n",
    "- Spot errors (e.g., duplicates, wrong labeling)\n",
    "- Check data distributions\n",
    "- Identify biases\n",
    "- Identify possible correct pre-processing steps\n",
    "- Helps in understanding model post-training evaluation (*error analysis*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee85f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Start simple\n",
    "\n",
    "We are not ready yet to plug-in our fancy model\n",
    "\n",
    "- Write a training/evaluation skeleton\n",
    "- Test simple baselines (*how far can they go?*) $\\rightarrow$ downplaying possible scripting errors\n",
    "- Avoid any unnecessary complexity $\\rightarrow$ pick the simplest setting possible\n",
    "- Overfit one single batch $\\rightarrow$ spotting errors, evaluate model capacity, evaluate data\n",
    "- Check learning curves\n",
    "- Inspect model prediction dynamics $\\rightarrow$ gives good intuition of model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014719e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfit\n",
    "\n",
    "Pick a large enough model that is able to overfit $\\rightarrow$ we understand that the task is *solvable* on training data\n",
    "\n",
    "- Pick existing models (*even their simplest version*) rather than making custom ones\n",
    "- Progressively introduce complexity $\\rightarrow$ multiple inputs, larger inputs, etc..\n",
    "- Pay attention to employed optimizers $\\rightarrow$ weight decay, learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e732f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularize\n",
    "\n",
    "Once we have a big enough model, we have to regularize it to allow better generalization capabilities\n",
    "\n",
    "- Add more data (*if you can!*) $\\rightarrow$ data-augmentation, real data, synthetic data\n",
    "- Check spurious inputs\n",
    "- Decrease model size\n",
    "- Decrease batch size (*if you are using batch normalization*)\n",
    "- Dropout\n",
    "- Weight decay\n",
    "- Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8db45b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning\n",
    "\n",
    "To find a good regularized model, we may consider a hyper-parameter calibration routine\n",
    "\n",
    "- Random search (*it usually works quite well to explore the hyper-parameter space*)\n",
    "- Bayesian optimization (e.g., Optuna)\n",
    "- Use your brain $\\rightarrow$ in many cases, you may need to think about the valid search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e6d48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f20b72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### File Organization\n",
    "\n",
    "Split your model into individual layers and losses to\n",
    "\n",
    "- Enhance re-usability (*easier to spot errors*)\n",
    "- Enhance readability (*top-down view of a model*)\n",
    "\n",
    "The same applies for nested models, layers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad35ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# losses.py\n",
    "class CustomLoss(th.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "        \n",
    "# layers.py\n",
    "class CustomLayer(th.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "        \n",
    "# models.py\n",
    "class CustomModel(th.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.layer = CustomLayer(...)\n",
    "        self.loss_op = CustomLoss(...)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a19db7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Tensorflow this is particularly recommended when considering ```tf.function```\n",
    "\n",
    "#### tf.function covers function nesting\n",
    "\n",
    "If a function invokes other functions, you just need to decorate the top-level function with ```tf.function```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95d072",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def loss_op(self, x, y, training=False):\n",
    "        output = self.model(x, training=training)\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "    def train_op(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_op(x=x, y=y, training=True)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_fit(self, x, y):\n",
    "        loss, grads = self.train_op(x, y)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_evaluate(self, x, y):\n",
    "        return self.loss_op(x=x, y=y, training=False)\n",
    "        \n",
    "    @tf.function\n",
    "    def batch_predict(self, x):\n",
    "        return self.model(x, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5db125",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential Layers\n",
    "\n",
    "In many cases, we may have to define a sequential network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f7866",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Which one do you use?\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-1-1.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-1-2.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-1-3.png\" width=\"1100\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(A)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(B)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(C)</strong> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d1272",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### (A) is the best choice\n",
    "\n",
    "- [**TH**] Uses nn.Sequential(...) to define a sequential network $\\rightarrow$ higher efficiency, readibility\n",
    "- [**TF**] Likewise, use ```tf.keras.Sequential``` in tensorflow\n",
    "    \n",
    "#### (B) may give some problems with list wrapping\n",
    "\n",
    "- [**TH**] Consider using ```th.nn.ModuleList(...)``` rather than a list\n",
    "- [**TF**] It is fine to list wrapping multiple layers, but the recommended way is to use ```tf.keras.Sequential```\n",
    "    \n",
    "#### (C) is terrible!\n",
    "\n",
    "- Generates layers at each forward pass $\\rightarrow$ losing track of model weights to update\n",
    "- You need to define layers in the ```__init__(...)``` method so that model weights are kept throughout the life the of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561da159",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calling layers\n",
    "\n",
    "Both Tensorflow and Pytorch implement layers as callable objects\n",
    "\n",
    "Always invoke your layer/model as ```layer(...)``` and ```model(...)```, respectively.\n",
    "\n",
    "#### Never invoke ```call(...)``` or ```forward(...)``` explicitly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1d94a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training and Evaluation modes\n",
    "\n",
    "Torch has two model modalities: ```model.train()``` and ```model.eval()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053051f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Which one do you use?\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-3-1.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-3-2.png\" width=\"1100\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(A)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(B)</strong> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef5e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Both are correct!\n",
    "\n",
    "#### Model.eval()\n",
    "\n",
    "- Just changes model execution so that layers like Dropout, BatchNorm can execute correctly\n",
    "\n",
    "#### torch.no_grad()\n",
    "\n",
    "- Disables automatic differentiation saving up memory and time\n",
    "\n",
    "\n",
    "In the common case where you don't compute any gradient during evaluation, you can use both to gain some speed-up and use less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6b932",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Beware of incorrect behaviours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb984e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, train_steps, train_data_iterator, val_steps, val_data_iterator):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_iterator = train_data_iterator()\n",
    "        for step in range(train_steps):\n",
    "            batch = next(epoch_train_iterator)\n",
    "            batch_loss = model.batch_fit(*batch)\n",
    "            \n",
    "        # Get loss on validation set\n",
    "        val_loss, val_metrics = evaluate(model=model, steps=val_steps, data_iterator=val_data_iterator)\n",
    "                \n",
    "                \n",
    "def evaluate(model, steps, data_iterator):\n",
    "    model.eval()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45979197",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tensorflow is as straightforward as Pytorch\n",
    "\n",
    "You just have to remember to call your model with ```training=True|False``` for training and evaluation modes, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fefe8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def loss_op(self, x, y, training=False):\n",
    "        output = self.model(x, training=training)  # <---\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "    def train_op(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_op(x=x, y=y, training=True)   # <---\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_fit(self, x, y):\n",
    "        loss, grads = self.train_op(x, y)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_evaluate(self, x, y):\n",
    "        return self.loss_op(x=x, y=y, training=False)   # <---\n",
    "        \n",
    "    @tf.function\n",
    "    def batch_predict(self, x):\n",
    "        return self.model(x, training=False)  # <---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead0ddf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixing operations\n",
    "\n",
    "Consider the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676d073",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy version\n",
    "loss = np.square(y_pred - y_true).sum()\n",
    "\n",
    "# Torch version\n",
    "loss = (y_pred - y_true).pow(2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbefa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The numpy code is always run on the CPU, while the torch code may also run on the GPU\n",
    "\n",
    "- Avoid mixing numpy and torch operations in ```forward(...)``` method since numpy operations slow down your code execution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f71b4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Detaching\n",
    "\n",
    "How to properly collect model outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baede4ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Which one do you use?\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-2-1.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-2-2.png\" width=\"1100\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(A)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(B)</strong> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70abf95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [TH] Detaching is need!\n",
    "\n",
    "- Remove tensor from torch tracking for automatic differentation\n",
    "- If you don't do that, the unnecessary recording of these tensors slows down your program execution!\n",
    "\n",
    "#### [TF] use ```tensor.numpy()```\n",
    "\n",
    "- If your operation is outside the tensorflow graph, you receive a ```tf.EagerTensor``` storing numerical content (based on provided inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3d73f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical Stability\n",
    "\n",
    "Mathematical correctness of your code doesn't necessarily translates to correct results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcf1b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Some examples\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-4-1.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-4-2.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-4-3.png\" width=\"1100\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(A)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(B)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(C)</strong> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb0998",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stable versions\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-5-1.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-5-2.png\" width=\"1100\"/> </td>\n",
    "<td> <img src=\"Images/Lecture-4/th-examples-5-3.png\" width=\"1100\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(A)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(B)</strong> </td>\n",
    "<td style=\"text-align: center; vertical-align: middle;\"> <strong>(C)</strong> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6e7a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shuffling data\n",
    "\n",
    "One common error is to not appropriately shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fd180",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Torch (torch_datapipe_example.py, Lines 92-96)\n",
    "data_generator = partial(self.light_iterator, df=df)\n",
    "data = IterableWrapper(data_generator(), deepcopy=False)\n",
    "\n",
    "if shuffle:\n",
    "    data = data.shuffle(buffer_size=100)    # <---\n",
    "    \n",
    "#------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Tensorflow (tf_data_pipeline_gen.py, Lines 65-72)\n",
    "data_generator = partial(self.light_iterator, df=df)\n",
    "data = tf.data.Dataset.from_generator(generator=data_generator,\n",
    "                                      output_signature=(\n",
    "                                          tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                          tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "                                      ))\n",
    "if shuffle:\n",
    "    data = data.shuffle(buffer_size=100)     # <---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbb8a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Beware of buffer_size\n",
    "\n",
    "These data pipeline APIs work by setting up a buffer from which sampling random examples.\n",
    "\n",
    "If your buffer is too small you may just sampling class-equivalent examples!\n",
    "\n",
    "- Always pre-shuffle your data (if possible)\n",
    "- Set a buffer_size equal to your data size (if not too big)\n",
    "- Inspect your data stream for sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e52b96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient clipping\n",
    "\n",
    "In many cases, you may want to clip gradients to increase model training stability\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-4/gradient_clipping.png\" width=\"1000\"/>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e7b33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=10)\n",
    "\n",
    "# Tensorflow\n",
    "grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "grads, _ = tf.clip_by_global_norm(grads, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0234c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.reshape/th.view vs tf.transpose/th.permute\n",
    "\n",
    "In many cases, we may erroneously use one operation in place of the other one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d377d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.range(6), [2, 3])\n",
    "# <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
    "#  array([[0, 1, 2], [3, 4, 5]], dtype=int32)>\n",
    "\n",
    "tf.reshape(x, [3, 2])\n",
    "# <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
    "# array([[0, 1],\n",
    "#        [2, 3],\n",
    "#        [4, 5]], dtype=int32)>\n",
    "\n",
    "tf.transpose(x)\n",
    "# <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
    "# array([[0, 3],\n",
    "#        [1, 4],\n",
    "#        [2, 5]], dtype=int32)>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587f535",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Beware of reshaping!\n",
    "\n",
    "If used incorrectly, it may lead to leakage between batch samples! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5744b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Squeezing\n",
    "\n",
    "Squeezing is the operation of removing 1-unit dimensions in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18babcd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shape -> [2, 3, 1, 1]\n",
    "x = tf.reshape(tf.range(6), [2, 3, 1, 1])\n",
    "\n",
    "# Shape -> [2, 3]\n",
    "x = tf.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c0316",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Beware of squeezing with batched tensors\n",
    "\n",
    "One time I lost a couple of hours with a strange error...\n",
    "\n",
    "It turned out that the batching with a specific ```batch_size``` led to a batch with a single sample\n",
    "\n",
    "$\\rightarrow$ squeezing without specifying any dimension inherently converted my input 3D tensors to 2D!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e119168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working with GPU devices\n",
    "\n",
    "When working with GPUs we have to carefully inspect how these devices are used\n",
    "\n",
    "$\\rightarrow$ this is particularly annoying with Tensorflow!\n",
    "\n",
    "- Tensorflow automatically reserves all available memory from the selected GPU\n",
    "- Moreover, Tensorflow also reserves some memory in all discovered GPUs, even if you are in a single-GPU setting (*efficiency reasons to reduce memory fragmentation*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a5453",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Enforces Tensorflow to just use the necessary amount of GPU memory\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Limiting visibility (only first gpu)\n",
    "gpu_start_index = 0\n",
    "gpu_end_index = 1\n",
    "tf.config.set_visible_devices(gpus[gpu_start_index:gpu_end_index], \"GPU\")\n",
    "\n",
    "# Setting max GPU memory (e.g., 3GB on first GPU)\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3072)])\n",
    "\n",
    "\n",
    "# Torch\n",
    "\n",
    "# Setting max GPU memory (e.g., 30% of total GPU memory)\n",
    "torch.cuda.set_per_process_memory_fraction(0.3, device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5e947",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Freeing GPU memory\n",
    "\n",
    "In many cases, you may find yourself in a scenario where multiple models have to be trained (e.g., cross-validation).\n",
    "\n",
    "In these cases, you may need to efficiently allocate/release GPU usage to avoid memory problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310777c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "from tensorflow.python.keras import backend as K\n",
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Torch\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf6776",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Manually freeing GPU memory may not be needed\n",
    "\n",
    "Generally speaking, Tensorflow/Torch might efficiently re-use the previously allocated memory\n",
    "\n",
    "$\\rightarrow$ manually freeing memory might lead to minor code execution speed reductions\n",
    "\n",
    "#### Tensorflow has a problematic GPU memory management\n",
    "\n",
    "According to this [thread](https://github.com/tensorflow/tensorflow/issues/36465), commands like ```K.clear_session()``` may not really work.\n",
    "\n",
    "Instead, the recommended way is to run your train/evaluation routine in a separate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f59f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "process_eval = multiprocessing.Process(target=evaluate, args=(...))\n",
    "process_eval.start()\n",
    "process_eval.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ae371",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus\n",
    "\n",
    "- Mixed-precision\n",
    "- Gradient accumulation\n",
    "- Distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fbfef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixed-precision\n",
    "\n",
    "In many cases, you may want to speed-up your training by relying on mixed-precision operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daa61d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Autocast\n",
    "with torch.cuda.amp.autocast():\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_op(outputs, targets)\n",
    "    \n",
    "# GradScaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "loss = ...\n",
    "optimizer = ...\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865318f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### torch.cuda.amp.autocast()\n",
    "\n",
    "- Automatically casts down heavy operations (e.g., convolution, matrix multiplication) to 16-bit\n",
    "- Allows mixed-precision computations\n",
    "\n",
    "\n",
    "#### torch.cuda.amp.GradScaler()\n",
    "\n",
    "- Allows to work with 16-bit gradient values while avoid under/over-flows\n",
    "- Scales up loss to avoid underflows\n",
    "- Scale gradient values down during gradient update to ensure correct model weights update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1dbb2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Tensorflow, mixed-precision is pretty easy to setup as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198ce6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1558391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aeb970",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef8ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Misc \n",
    "\n",
    "- Code documentation\n",
    "- Writing a proper README\n",
    "- Controlled environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7a733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ac401",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Writing a proper README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee32a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Controlled environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b8a33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 次回 (Jikai!)\n",
    "\n",
    "Actually, there's nothing left to show you...\n",
    "\n",
    "Since I wanted to hold a 10-hours course (thus, 2 CFUs), I thought it could have been a good opportunity to show you something I've been working on.\n",
    "\n",
    "- Deasy-learning (*a tiny tiny custom library for research*)\n",
    "- Course feedback (*don't forget to leave a like and hit subscribe!* ~semicit)\n",
    "- **Motivational outro** (*please, don't miss this!*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3256c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any questions?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-1/jojo-arrivederci.gif\" width=\"1200\" alt='JOJO_arrivederci'/>\n",
    "</div>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
