{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb046532",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About last time\n",
    "\n",
    "- Reproducibility (computational, of findings) issues affect a wide variety of research papers\n",
    "\n",
    "- A recent and emerging trend is pushing towards open research communities\n",
    "\n",
    "- Guidelines, datasheets, model sheets, checklists are proposed to address reproducibility issues\n",
    "\n",
    "- For several issues like data leakage there exist specific solutions\n",
    "\n",
    "- Coding plays a crucial part in defining reproducible and robust experiments\n",
    "    - Reproducible: same data and tools\n",
    "    - Robust: same data but different tools (e.g., code re-implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabd5d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A quick Legend (memo)\n",
    "\n",
    "[‚ùì] $\\rightarrow$ This is a **question for you** (the audience)\n",
    "\n",
    "[‚ùó] $\\rightarrow$ A **very important** information to remember!\n",
    "\n",
    "[üí¨] $\\rightarrow$ An (funny?) **anecdote** will be told\n",
    "\n",
    "[[XX et al., YEAR](https://www.youtube.com/watch?v=dQw4w9WgXcQ)] $\\rightarrow$ A reference (**click on the reference** to see the paper) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e60dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## About this lecture\n",
    "\n",
    "We explore reproducibility issues and data leakage more in detail\n",
    "\n",
    "- Dataset creation\n",
    "\n",
    "- Data processing\n",
    "\n",
    "- Model setup\n",
    "\n",
    "- Reporting\n",
    "\n",
    "- **Coding**: the next 3 lectures are about this!\n",
    "\n",
    "We are going to consider some real examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e679a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset creation\n",
    "\n",
    "What might go wrong? How to follow a correct approach?\n",
    "\n",
    "- Initial motivation: do we really need to create a new dataset?\n",
    "\n",
    "- What data to collect?\n",
    "\n",
    "- For what purpose(s)?\n",
    "\n",
    "- How to collect data? annotators' selection, training, guidelines definition, pilot studies\n",
    "\n",
    "- How to state if a data collection was 'successful'? Size, quality, coverage, biases, unbalance\n",
    "\n",
    "- Ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ffb0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Partitioning\n",
    "\n",
    "Data partitioning is a crucial step since model performance is computed on built dev and test splits.\n",
    "\n",
    "Suppose you have some available data (e.g., you created it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa5273",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] How should we split data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723ae90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] For what purpose are we splitting data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cb940",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is common practice to collect a dataset and build train, dev and test splits\n",
    "\n",
    "These splits are usually built by following a specific criterion\n",
    "\n",
    "- Order of collection\n",
    "- Time order\n",
    "- Metadata (e.g., different speakers)\n",
    "- Random !\n",
    "\n",
    "These splits are formally known as **standard splits**\n",
    "\n",
    "This process is perfectly fine: we have the data, we follow the splits and train/evaluate our models of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301a1c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Are standard splits good?\n",
    "\n",
    "[‚ùó] [[Gorman & Bedrick, 2019](https://aclanthology.org/P19-1267.pdf)] showed that there's a relevant discrepancy between standard and random splits\n",
    "\n",
    "In particular, they argue in favor of random splits to have better estimate of model performance on new data samples (i.e., an hypothetical real-world scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dd6f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, [[S√∏gaard et al., 2021](https://aclanthology.org/2021.eacl-main.156v2.pdf)] showed that random split (and other data partitioning approaches) yet provide an over-estimate of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b0fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/talk_about_random_splits.png\" width=\"2000\" alt='talk_about_random_splits'/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "- Random and standard splits under-estimate error on new samples\n",
    "- Heuristic and adversarial splits sometime under-estimate error on new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f5433",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What should we do?\n",
    "\n",
    "[[S√∏gaard et al., 2021](https://aclanthology.org/2021.eacl-main.156v2.pdf)] propose to:\n",
    "\n",
    "1. Consider/build multiple diverse test splits with different biases $\\rightarrow$ better evaluation\n",
    "2. If (1) is not possible, it is better to consider biased splits to better approximate real-world performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa4d36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The importance of biases\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-2/different_time_samples.png\" width=\"1100\" alt='different_time_samples'/> </td>\n",
    "<td> <img src=\"Images/Lecture-2/temporal_drift.png\" width=\"1100\" alt='temporal_drift'/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13544def",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seeding\n",
    "\n",
    "Pseudo random number generation is a critical aspect in developing experiments.\n",
    "\n",
    "Choosing a random seed fixes the pseudo random number generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e80083",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] What processes are affected by a random seed? Have you ever fixed seeds?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173d79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Neural network initialization\n",
    "- Optimization\n",
    "- Neural network prediction\n",
    "- Data pipeline (e.g., data partitioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c23676",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[‚ùì] How should we use random seeds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2484c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Model selection $\\rightarrow$ hyper-parameters calibration\n",
    "2. Ensemble creation\n",
    "3. Sensitivity analysis\n",
    "4. Single fixed seed $\\rightarrow$ through all model pipeline\n",
    "5. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c190510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "According to [[Bethard, 2022](https://arxiv.org/pdf/2210.13393.pdf)], based on an analysis of 85 ACL papers, points [1-3] are safe approaches, while points [4-5] hide some critical risks.\n",
    "\n",
    "In particular, Bethard found that 48 out of 85 papers follow risky approaches. \n",
    "\n",
    "This trend is unaffected by time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796e070",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "A random seed is just another hyper-parameter of the model.\n",
    "\n",
    "It is perfectly fine to find for the best random seeds like for other hyper-parameters.\n",
    "\n",
    "*\"...Training multiple models with randomized initializations and use as the final model the one which achieved the best performance on the validation set''* [[Bj√∂rne & Salakoski, 2018](https://aclanthology.org/W18-2311.pdf)]\n",
    "\n",
    "*\"The test results are derived from the 1-best random seed on the validation set''* [[Kuncoro et al., 2020](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00345/96469/Syntactic-Structure-Distillation-Pretraining-for)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c17180",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ensemble creation\n",
    "\n",
    "Ensembling is a standard strategy to achieve increased performance by combining multiple models\n",
    "\n",
    "One can create an ensemble by training the same model with different random seeds and obtain predictions via voting\n",
    "\n",
    "*\"...We ensemble five distinct models each initialized with a different random seed''* [[Nicolai et al., 2017](https://aclanthology.org/K17-2008.pdf)]\n",
    "\n",
    "*\" Our model is composed of the ensemble of 8 single models. The hyperparameters and the training procedure used in each single model are the same except the random seed''* [[Yang & Wang, 2019](https://aclanthology.org/W19-4421.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2bbe1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Sensitivity Analysis\n",
    "\n",
    "Since neural networks may be sensitive to random initialization, one could study this aspect by considering multiple seed runs\n",
    "\n",
    "*\"...examine the expected variance in attention-procuded weights by initializing multiple training sequences with different random seeds''* [[Wiegreffe & Pinter, 2019](https://aclanthology.org/D19-1002.pdf)]\n",
    "\n",
    "*\"Our model shows a lower standard deviation on each task, which means our model is less sensitive to random seeds than other models''* [[Hua et al., 2021](https://aclanthology.org/2021.naacl-main.258.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd2af4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single fixed seed\n",
    "\n",
    "Pick a single random seed to fix the whole model pipeline to ensure reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef254394",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] Why is this risky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf201f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Doesn't necessarily guarantee reproducibility $\\rightarrow$ some libraries (e.g., Tensorflow) may not support seed fixing for all operations (it also depends on the library version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c366af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Seed fixing implies no calibration for such a hyper-parameter $\\rightarrow$ performance under-estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe45553",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What should be done instead\n",
    "\n",
    "- Calibrate the random seed like any other hyper-parameter [[Dodge et al., 2020](https://arxiv.org/pdf/2002.06305.pdf)]\n",
    "- Low-resource scenario? Random search [[Bergstra & Bengio, 2012](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff8a58",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance comparison\n",
    "\n",
    "For model comparison, one should prefer considering distributions of model performance and not single point estimates to draw more reliable conclusions.\n",
    "\n",
    "[‚ùó] However, the trend is to just consider random seeds variations to obtain such distributions\n",
    "\n",
    "*\"We re-ran both implementations multiple times, each time only changing the seed value of the random number generator''* [[Reimers & Gurevych, 2017](https://aclanthology.org/D17-1035.pdf)]\n",
    "\n",
    "*\"Indeed, the best approach is to stop reporting single-value results, and instead report the distribution of results from a range of seeds. Doing so allows for a fairer comparison across models''* [[Crane, 2018](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00018/43441/Questionable-Answers-in-Question-Answering)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ad787",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[‚ùì] Why is this risky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a1dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Sub-optimal models if the goal is to compare the best possible model A from one architecture to the best possible model B from another architecture (e.g., leaderboards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19659c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Biased slice of family if the goal is to compare the family of models A to the family of models B $\\rightarrow$ we should consider all hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40682fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What should be done instead\n",
    "\n",
    "- For (1), we should also optimize for the best possible seed $\\rightarrow$ we can still consider distributions rather than point estimates by considering multiple test splits.\n",
    "- For (2), we should sample different models from a family by considering all hyper-parameters and not just random seeds."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
