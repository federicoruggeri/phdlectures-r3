{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb046532",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About last time\n",
    "\n",
    "- Reproducibility (computational, of findings) issues affect a wide variety of research papers\n",
    "\n",
    "- A recent and emerging trend is pushing towards open research communities\n",
    "\n",
    "- Guidelines, datasheets, model sheets, checklists are proposed to address reproducibility issues\n",
    "\n",
    "- For several issues like data leakage there exist specific solutions\n",
    "\n",
    "- Coding plays a crucial part in defining reproducible and robust experiments\n",
    "    - Reproducible: same data and tools\n",
    "    - Robust: same data but different tools (e.g., code re-implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabd5d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A quick Legend (memo)\n",
    "\n",
    "[‚ùì] $\\rightarrow$ This is a **question for you** (the audience)\n",
    "\n",
    "[‚ùó] $\\rightarrow$ A **very important** information to remember!\n",
    "\n",
    "[üí¨] $\\rightarrow$ An (funny?) **anecdote** will be told\n",
    "\n",
    "[[XX et al., YEAR](https://www.youtube.com/watch?v=dQw4w9WgXcQ)] $\\rightarrow$ A reference (**click on the reference** to see the paper) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e60dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## About this lecture\n",
    "\n",
    "We explore reproducibility issues and data leakage more in detail\n",
    "\n",
    "- Dataset creation\n",
    "\n",
    "- Data processing\n",
    "\n",
    "- Model setup\n",
    "\n",
    "- Reporting\n",
    "\n",
    "- **Coding**: the next 3 lectures are about this!\n",
    "\n",
    "We are going to consider some real examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e679a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset creation\n",
    "\n",
    "What might go wrong? How to follow a correct approach?\n",
    "\n",
    "- Initial motivation: do we really need to create a new dataset?\n",
    "\n",
    "- What data to collect?\n",
    "\n",
    "- For what purpose(s)?\n",
    "\n",
    "- How to collect data? annotators' selection, training, guidelines definition, pilot studies\n",
    "\n",
    "- How to state if a data collection was 'successful'? Size, quality, coverage, biases, unbalance\n",
    "\n",
    "- Ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ffb0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Partitioning\n",
    "\n",
    "Data partitioning is a crucial step since model performance is computed on built dev and test splits.\n",
    "\n",
    "Suppose you have some available data (e.g., you created it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa5273",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] How should we split data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723ae90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] For what purpose are we splitting data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cb940",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is common practice to collect a dataset and build train, dev and test splits\n",
    "\n",
    "These splits are usually built by following a specific criterion\n",
    "\n",
    "- Order of collection\n",
    "- Time order\n",
    "- Metadata (e.g., different speakers)\n",
    "- Random !\n",
    "\n",
    "These splits are formally known as **standard splits**\n",
    "\n",
    "This process is perfectly fine: we have the data, we follow the splits and train/evaluate our models of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301a1c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Are standard splits good?\n",
    "\n",
    "[‚ùó] [[Gorman & Bedrick, 2019](https://aclanthology.org/P19-1267.pdf)] showed that there's a relevant discrepancy between standard and random splits\n",
    "\n",
    "In particular, they argue in favor of random splits to have better estimate of model performance on new data samples (i.e., an hypothetical real-world scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dd6f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, [[S√∏gaard et al., 2021](https://aclanthology.org/2021.eacl-main.156v2.pdf)] showed that random split (and other data partitioning approaches) yet provide an over-estimate of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b0fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/talk_about_random_splits.png\" width=\"2000\" alt='talk_about_random_splits'/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "- Random and standard splits under-estimate error on new samples\n",
    "- Heuristic and adversarial splits sometime under-estimate error on new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f5433",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What should we do?\n",
    "\n",
    "[[S√∏gaard et al., 2021](https://aclanthology.org/2021.eacl-main.156v2.pdf)] propose to:\n",
    "\n",
    "1. Consider/build multiple diverse test splits with different biases $\\rightarrow$ better evaluation\n",
    "2. If (1) is not possible, it is better to consider biased splits to better approximate real-world performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc1d71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Is train-dev-test partitioning correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c77271",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[[Rob van der Goot](https://aclanthology.org/2021.emnlp-main.368.pdf)] notices\n",
    "\n",
    "- A clear mismatch between classical and neural models\n",
    "- Over-estimations on dev splits limit evaluations on test splits only $\\rightarrow$ overfitting and expiration of test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6ffeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfitting: design decisions (e.g., hyper-parameters) $\\rightarrow$ *bias from research design* [[Hovy & Prabhumoye, 2021](https://compass.onlinelibrary.wiley.com/doi/epdf/10.1111/lnc3.12432)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823ee41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Model comparison\n",
    "\n",
    "If we want to compare model A to model B, we calibrate A and B on the dev split and then evaluate them on the test split. $\\rightarrow$ model architecture search\n",
    "\n",
    "In particular, we pick the best-epoch for each model based on the dev split $\\rightarrow$ **early stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07c6f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dev split over-estimation\n",
    "\n",
    "If we calibrate on the dev split $\\rightarrow$ overly optimistic performance on the dev split\n",
    "\n",
    "Thus, regarding model picking\n",
    "\n",
    "- If on the dev split: over-estimation since model picking and calibration are both done on the dev split\n",
    "- If on the test split: overfitting of design decisions, leading to faster obsolescence of the test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3d655",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What can be done?\n",
    "\n",
    "Introducing a *tune split*\n",
    "\n",
    "- Early stop on tune split\n",
    "- Hyper-parameters calibration on dev split\n",
    "- Validate results on test split\n",
    "\n",
    "This also allows a fair comparison with classical models since they don't use a dev split during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc67a67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Downsides\n",
    "\n",
    "- More data required!\n",
    "- It is possible to add tune split to train data for final evaluation on the test split (early stop on dev)\n",
    "    - Similar to shared tasks: use train + dev\n",
    "    - Similar to cross-lingual/domain setups: source dataset dev split for model picking $\\rightarrow$ don't use all target languages [[Artetxe et al., 2020](https://aclanthology.org/2020.acl-main.658.pdf)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa4d36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The importance of biases\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-2/different_time_samples.png\" width=\"1100\" alt='different_time_samples'/> </td>\n",
    "<td> <img src=\"Images/Lecture-2/temporal_drift.png\" width=\"1100\" alt='temporal_drift'/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13544def",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seeding\n",
    "\n",
    "Pseudo random number generation is a critical aspect in developing experiments.\n",
    "\n",
    "Choosing a random seed fixes the pseudo random number generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e80083",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] What processes are affected by a random seed? Have you ever fixed seeds?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173d79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Neural network initialization\n",
    "- Optimization\n",
    "- Neural network prediction\n",
    "- Data pipeline (e.g., data partitioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c23676",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[‚ùì] How should we use random seeds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2484c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Model selection $\\rightarrow$ hyper-parameters calibration\n",
    "2. Ensemble creation\n",
    "3. Sensitivity analysis\n",
    "4. Single fixed seed $\\rightarrow$ through all model pipeline\n",
    "5. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c190510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "According to [[Bethard, 2022](https://arxiv.org/pdf/2210.13393.pdf)], based on an analysis of 85 ACL papers, points [1-3] are safe approaches, while points [4-5] hide some critical risks.\n",
    "\n",
    "In particular, Bethard found that 48 out of 85 papers follow risky approaches. \n",
    "\n",
    "This trend is unaffected by time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796e070",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "A random seed is just another hyper-parameter of the model.\n",
    "\n",
    "It is perfectly fine to find for the best random seeds like for other hyper-parameters.\n",
    "\n",
    "*\"...Training multiple models with randomized initializations and use as the final model the one which achieved the best performance on the validation set''* [[Bj√∂rne & Salakoski, 2018](https://aclanthology.org/W18-2311.pdf)]\n",
    "\n",
    "*\"The test results are derived from the 1-best random seed on the validation set''* [[Kuncoro et al., 2020](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00345/96469/Syntactic-Structure-Distillation-Pretraining-for)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c17180",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ensemble creation\n",
    "\n",
    "Ensembling is a standard strategy to achieve increased performance by combining multiple models\n",
    "\n",
    "One can create an ensemble by training the same model with different random seeds and obtain predictions via voting\n",
    "\n",
    "*\"...We ensemble five distinct models each initialized with a different random seed''* [[Nicolai et al., 2017](https://aclanthology.org/K17-2008.pdf)]\n",
    "\n",
    "*\" Our model is composed of the ensemble of 8 single models. The hyperparameters and the training procedure used in each single model are the same except the random seed''* [[Yang & Wang, 2019](https://aclanthology.org/W19-4421.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2bbe1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Sensitivity Analysis\n",
    "\n",
    "Since neural networks may be sensitive to random initialization, one could study this aspect by considering multiple seed runs\n",
    "\n",
    "*\"...examine the expected variance in attention-procuded weights by initializing multiple training sequences with different random seeds''* [[Wiegreffe & Pinter, 2019](https://aclanthology.org/D19-1002.pdf)]\n",
    "\n",
    "*\"Our model shows a lower standard deviation on each task, which means our model is less sensitive to random seeds than other models''* [[Hua et al., 2021](https://aclanthology.org/2021.naacl-main.258.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e36ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-2/tune_split_test_performance.png\" width=\"1100\" alt='tune_split_test_performance'/> </td>\n",
    "<td> <img src=\"Images/Lecture-2/tune_split_dev_test_performance.png\" width=\"1100\" alt='tune_split_dev_test_performance'/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd2af4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single fixed seed\n",
    "\n",
    "Pick a single random seed to fix the whole model pipeline to ensure reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef254394",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[‚ùì] Why is this risky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf201f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Doesn't necessarily guarantee reproducibility $\\rightarrow$ some libraries (e.g., Tensorflow) may not support seed fixing for all operations (it also depends on the library version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c366af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Seed fixing implies no calibration for such a hyper-parameter $\\rightarrow$ performance under-estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe45553",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What should be done instead\n",
    "\n",
    "- Calibrate the random seed like any other hyper-parameter [[Dodge et al., 2020](https://arxiv.org/pdf/2002.06305.pdf)]\n",
    "- Low-resource scenario? Random search [[Bergstra & Bengio, 2012](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff8a58",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance comparison\n",
    "\n",
    "For model comparison, one should prefer considering distributions of model performance and not single point estimates to draw more reliable conclusions.\n",
    "\n",
    "[‚ùó] However, the trend is to just consider random seeds variations to obtain such distributions\n",
    "\n",
    "*\"We re-ran both implementations multiple times, each time only changing the seed value of the random number generator''* [[Reimers & Gurevych, 2017](https://aclanthology.org/D17-1035.pdf)]\n",
    "\n",
    "*\"Indeed, the best approach is to stop reporting single-value results, and instead report the distribution of results from a range of seeds. Doing so allows for a fairer comparison across models''* [[Crane, 2018](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00018/43441/Questionable-Answers-in-Question-Answering)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ad787",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[‚ùì] Why is this risky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a1dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Sub-optimal models if the goal is to compare the best possible model A from one architecture to the best possible model B from another architecture (e.g., leaderboards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19659c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Biased slice of family if the goal is to compare the family of models A to the family of models B $\\rightarrow$ we should consider all hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40682fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What should be done instead\n",
    "\n",
    "- For (1), we should also optimize for the best possible seed $\\rightarrow$ we can still consider distributions rather than point estimates by considering multiple test splits.\n",
    "- For (2), we should sample different models from a family by considering all hyper-parameters and not just random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47854a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical significance\n",
    "\n",
    "A standard tool to assess that experimental results are not coincidental.\n",
    "\n",
    "[‚ùì] Which statistical tool should we use? Under which conditions should we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934dc0a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[[Dror et al., 2018](https://aclanthology.org/P18-1128.pdf)] surveyed ~200 papers from ACL'17 and found that statistical significance is often not reported or wrongly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28685f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/dror_survey.png\" width=\"700\" alt='dror_survey'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9700689",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our goal is\n",
    "\n",
    "Make sure the difference between two algorithms on a single comparison is not coincidental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87a7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "We have two algorithms: $A$ and $B$\n",
    "\n",
    "We have a dataset $X$\n",
    "\n",
    "We have an evaluation measure $\\mathcal{M}$ $\\rightarrow$ $\\mathcal{M}(\\cdot, X)$ is its value on dataset $X$ for algorithm $\\cdot$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c4bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define the difference in performance between $A$ and $B$ as:\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{eq:delta}\n",
    "    \\delta(X) = \\mathcal{M}(A, X) - \\mathcal{M}(B, X)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fd062",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the stastical hypothesis testing problem is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_0: \\delta(X) \\le 0 \\\\\n",
    "    H_1: \\delta(X) > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We want to give very low probability for $H_0$ being true in order to reject it and accept our desired hypothesis $H_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d565bcc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do so, we compute the p-value: the probability, under $H_0$, of obtaining a result equal to or more extreme than what was actually observed:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P \\left ( \\delta(Y) \\ge \\delta_{observed} | H_0 \\right ) \\quad (\\delta_{observed} \\, \\text{is derived from Eq. \\ref{eq:delta}})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $Y$ is a random variable over possible observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8ea9b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The smaller the p-value, the higher the significance $\\rightarrow$ H_0 does not hold\n",
    "\n",
    "To reject $H_0$ one should define a threshold $\\alpha$, the significance level: reject only if p-value $< \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ad032",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Error types\n",
    "\n",
    "- Type I: $H_0$ is rejected when it is actually true\n",
    "- Type II: $H_0$ is not rejected although it should be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00740f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric vs non-parametric tests\n",
    "\n",
    "Statistical significance depends on two notions: $\\mathcal{M}$ and the distribution of $\\delta{X}$\n",
    "\n",
    "- If the distribution of $\\delta(X)$ is known (we call the distribution parameters $\\theta$): parametric tests are the way to ensure low probability of Type II error\n",
    "- Otherwise, we rely on non-parametric tests (less powerful but statistically sound)\n",
    "\n",
    "#### How to know the distribution of $\\delta{X}$?\n",
    "\n",
    "- Shapiro-Wilk test\n",
    "- Kolmogorov-Smirnov test\n",
    "- Anderson-Darling test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1986dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric tests\n",
    "\n",
    "Typically assume the normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f0695",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Paired Student's t-test\n",
    "\n",
    "Assesses whether the population means of two sets of measurements differ from each other\n",
    "\n",
    "Assumes that both samples come from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36614e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### When is it applied?\n",
    "\n",
    "On evaluation measures like accuracy, UAS and LAS $\\rightarrow$ compute the mean of correct predictions per example $\\rightarrow$ structured tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fa91a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Usually based on the idea of the Central Limit Theorem $\\rightarrow when the number of indivial predictions (e.g., words in a sentence) is large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33ff7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-parametric tests\n",
    "\n",
    "There are two families\n",
    "\n",
    "- Sampling-free: Does not consider the actual values of the evaluation measures\n",
    "- Sampling-based: Does consider the values of the measures\n",
    "\n",
    "#### Sampling-free\n",
    "\n",
    "Only considers the number of cases in which each of the algorithms performs better than the other\n",
    "\n",
    "Lower statistical power than sampling-based but far less computationally intensive\n",
    "\n",
    "- Sign test\n",
    "- McNemar's test / Cochran's Q test\n",
    "- Wilcoxon signed-rank test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8618b30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sign test\n",
    "\n",
    "Test statistic: number of examples for which A is better than B but ignores the extent of the difference!\n",
    "\n",
    "Null hypothesis: given a new pair of measurements (a_i, b_i) then a_i and b_i are equally likely to be larger than the other\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- Data samples are i.i.d.\n",
    "- The differences come from a continuous distribution\n",
    "- The values are ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d2875",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### McNemar's test\n",
    "\n",
    "A special case of sign test for binary classification\n",
    "\n",
    "Null hypothesis: marginal probability for each outcome is the same for both A and B $\\rightarrow A and B are expected to make the same proportions of correct/incorrect predictions\n",
    "\n",
    "The Cochran's Q test generalizes for multi-class classification setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57df39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Wilcoxon's signed-rank test\n",
    "\n",
    "More powerful than previous methods\n",
    "\n",
    "Null hypothesis: the differences follow a symmetric distribution around zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24a725",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sampling-based\n",
    "\n",
    "- Permutation/randomization test\n",
    "- Paired bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5a8dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Pitman's permutation test\n",
    "\n",
    "Computes statistical significance under all possible labellings (permutations) of the test set\n",
    "   \n",
    "   - Compute original sum of differences between A and B: $d_0$\n",
    "   - Repeat $R$ times\n",
    "   - Randomly swap $\\mathcal{M}(A, x_i)$ with $\\mathcal{M}(B, x_i)$\n",
    "   - Compute sum of differences between A and B: $d_r$, $r \\in 1..R$\n",
    "   - The p-value is the ratio of times where $d_r \\le d_0$  \n",
    "\n",
    "Exponentially large number of possible permutations\n",
    "\n",
    "In pratice, we use approximations: a pre-defined limited number of permutations are drawn without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f0a7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paired bootstrap test\n",
    "\n",
    "Differs from Pitman's test by allowing replacements: an example from the original test data can appear more than once in a sample)\n",
    "\n",
    "Uses the samples as surrogates populations for the purpose of approximating the sampling distribution of the statistic\n",
    "\n",
    "Less effective for small test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946e861",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which one to use?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/statistical_significance.png\" width=\"1500\" alt='statistical_significance'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eecd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Open problems\n",
    "\n",
    "#### Dependent observations \n",
    "\n",
    "In many cases, samples are not i.i.d. (e.g., sentences from the same document)\n",
    "\n",
    "Hard to quantify the nature of the dependence between (test) samples\n",
    "    \n",
    "#### Cross-validation\n",
    "\n",
    "Test splits of different folds are not independent\n",
    "\n",
    "A possible solution is to use $K_{Bonferroni}$ estimator [[Dror et al., 2017](https://aclanthology.org/Q17-1033.pdf)]\n",
    "  1. Calculate p-value for each fold separately\n",
    "  2. Perform replicability analysis for dependent datasets with $K_{Bonferroni}$\n",
    "  3. If the analysis rejects the null hypothesis in all folds the results should be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8417d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replicability analysis\n",
    "\n",
    "In many cases we experimentally compare a model A to another model B on several datasets\n",
    "\n",
    "However, aggregating individual statistical significance tests over multiple datasets is error prone: the probability of making one or more false claims is very high!\n",
    "\n",
    "[[Dror et al., 2017](https://aclanthology.org/Q17-1033.pdf)] propose a method for\n",
    "\n",
    "- Counting: for how many datasets does a given algorithm outperform another?\n",
    "- Identification: what are these datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e31b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Counting\n",
    "\n",
    "Recall the statistical hypothesis testing problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_0: \\delta(X) \\le 0 \\\\\n",
    "    H_1: \\delta(X) > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we have $N$ datasets, we have to test for rejecting $H_0$ $N$ times! $\\rightarrow$ it is likely to make some erroneous rejections!\n",
    "\n",
    "For instance, if $\\alpha = 0.05$, we have 5% chance to make an erroneous rejection. If $N = 100$, we can expect to make around 5 wrong rejections.\n",
    "\n",
    "The probability of making at least one erroneous rejection is $1 - (1 - 0.05)^{100} = 0.994$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ac9c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Partial conjunction test\n",
    "\n",
    "We consider different $H_0$ and H_1$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_0^{u/N}: k < u \\\\\n",
    "    H_1^{u/N}: k \\ge u\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $k$ is the true unknown number of false null hypotheses $\\rightarrow$ number of datasets where A is truly better than B\n",
    "\n",
    "This problem translates to *\"Are at least $u$ out of $N$ null hypotheses false?''*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5268ba0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two estimators for $k$ (i.e., for addressing counting)\n",
    "\n",
    "- Bonferroni: for dependent datasets\n",
    "- Fischer: for independent datasets (requires independence but more powerful than Bonferroni)\n",
    "\n",
    "By computing the estimator, we can an estimate our $\\hat{k}$, meaning that A is better than B in at least $\\hat{k}$ datasets with a confidence level $1 - \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f25a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identification\n",
    "\n",
    "A naive approach would consist in finding the datasets with p-value below the nominal significance level $\\rightarrow$ error-prone!\n",
    "\n",
    "One could use the Bonferroni correction: $\\alpha = \\alpha / N$ $\\rightarrow$ very strict threshold!\n",
    "\n",
    "#### Holm procedure\n",
    "\n",
    "- pick $k$ (minimal index) such that $p_k > \\frac{\\alpha}{N + 1 - k}$\n",
    "- reject the first $k$ null hypotheses\n",
    "- if no such $k$ can be found, then reject all null hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9cc20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparisons\n",
    "\n",
    "Consider $\\hat{k}_{count}$ as the estimator of $k$ that counts the number of datasets for which the difference between A and B is significant with p-value $\\le \\alpha$\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"Images/Lecture-2/k_estimator_simulation.png\" width=\"1100\" alt='k_estimator_simulation'/> </td>\n",
    "<td> <img src=\"Images/Lecture-2/k_estimator_real.png\" width=\"1100\" alt='k_estimator_real'/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e570b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is null-hypothesis significance test the only option?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/comparison.png\" width=\"1000\" alt='comparison'/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "What is the correct way to interpret empirical observation in terms of the superiority of one system over another?\n",
    "\n",
    "While $S_1$ has higher accuracy than $S_2$ in both cases, the gap is moderate and the datasets are of limited size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62062d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Azer et al., 2020](https://aclanthology.org/2020.acl-main.506.pdf)] make at least 4 statistically distinct hypotheses, each supported by a different statistical evaluation\n",
    "\n",
    "#### H1 (null hypothesis)\n",
    "\n",
    "**Assuming** $S_1$ and $S_2$ have inherently **identical** accuracy, the probability (p-value) of making a hypothetical observation with an accuracy gap at least as large as the empirical observation (here, 3.5%) is at most 5% (making us 95% confident that the above assumption is false).\n",
    "\n",
    "#### H2 (confidence intervals)\n",
    "\n",
    "**Assuming** $S_1$ and $S_2$ have inherently **identical** accuracy, the empirical accuracy gap (here, 3.5%) is\n",
    "larger than the maximum possible gap (confidence interval) that could hypothetically be observed with a\n",
    "probability of over 5% (making us 95% confident that the above assumption is false).\n",
    "\n",
    "#### H3 (posterior intervals)\n",
    "\n",
    "**Assume a prior belief** (a probability distribution) w.r.t. the inherent accuracy of typical systems. Given the\n",
    "empirically observed accuracies, the **probability** (posterior interval) that the inherent accuracy of S1 exceeds that of S2 **by a margin** of 1% is at least 95%.\n",
    "\n",
    "#### H4 (Bayes factor)\n",
    "\n",
    "**Assume a prior belief** (a probability distribution) w.r.t. the inherent accuracies of typical systems. Given the empirically observed accuracies, **the odds** increase by a factor of 1.32 (Bayes factor) in favor of the hypothesis that the inherent accuracy of S1 exceeds that of S2 **by a margin** of 1%\n",
    "\n",
    "H1 and H2 can be tested with p-value-based methods $\\rightarrow$ operate over the probability space of a test statistics ($\\delta$) over observations\n",
    "\n",
    "H3 and H4 are based on Bayesian inference $\\rightarrow$ operate directly over the probability space of inherent accuracy (rather than observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e6d23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Due to time reasons, we don't explore all these methods in details.\n",
    "\n",
    "I recommend further reading [[Azer et al., 2020](https://aclanthology.org/2020.acl-main.506.pdf)] as it clearly describes all possible misconceptions, misuses and statistical tools when comparing two models on some observed data.\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/azer_survey.png\" width=\"1000\" alt='azer_survey'/>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e93d3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Takeaways\n",
    "\n",
    "1. p-values do not provide probability estimates on two systems being different (or equal). They can only allow you to conclude that one system is better than the other without taking into account the extent of the difference between the systems (binary thinking)\n",
    "\n",
    "\n",
    "2. A common misconception is: *\"if p-value $< 0.05$, the null hypothesis has only a 5% chance of being true''* $\\rightarrow$ this is false since we are defining p-value with the assumption of null hypothesis being true\n",
    "\n",
    "\n",
    "3. Another misconception is: *\"if p-value $> 0.05$, there is no difference between the two systems''* $\\rightarrow$ a large p-value only means that the null hypothesis is consistent with observations. It does not tell you anything about the likeliness of the null hypothesis\n",
    "\n",
    "\n",
    "4. Another misconception is: *\"a statistically significant result (p-value $< 0.05$) indicates a notable difference between the two systems''* $\\rightarrow$ p-value only indicates strict superiority and provides no information about the margin of the effect \n",
    "\n",
    "\n",
    "5. Posterior intervals generally provide a useful summary as they capture probabilistic estimates of the correctness of the hypothesis. For instance, we can say \"with probability 0.95, model A's accuracy is two percent higher than that of B $\\rightarrow$ A outperforms B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3ea1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sticking to widely known reporting standards\n",
    "\n",
    "[[Marie et al., 2021](https://aclanthology.org/2021.acl-long.566.pdf)] analyze 769 papers concerning machine translation and observe that\n",
    "\n",
    "- BLEU metric is used as a reference metric in 99% of the papers, while there are more than 100 metrics that better correlate with human judgements than BLEU $\\rightarrow$ different metric choice leads to different model rankings!\n",
    "\n",
    "- Significance testing is only used in 65% of the papers\n",
    "\n",
    "- Just considering 2019-2020, 40% of the papers copiede results of previous work $\\rightarrow$ it is often unclear whether copied and proper results are comparable! (metrics may have parameters)\n",
    "    - For instance, a metric may depend on the tokenizer used and the pre-processing pipeline\n",
    "    \n",
    "- Just considering 2019-2020, 38% of the papers claimed superiority of a particular method but used different data (e..g, pre-processing pipeline)\n",
    "\n",
    "More in general, there are several factors that may lead to a method being perceived as superior to another: *benchmark lottery* [[Dehghani et al., 2021](https://arxiv.org/pdf/2107.07002.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82ca4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aggregating results via averaging\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/BT_example.png\" width=\"1000\" alt='BT_example'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b1fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Peyrard et al., 2021](https://aclanthology.org/2021.acl-long.179.pdf)] argue that averaging across samples on the same test set for comparing two models is not a robust approach\n",
    "\n",
    "They propose the Bradley-Terry (BT) model to account for sample-level pairing: compares systems for each test instance and estimates the latent strength of systems based on how frequently one system scores higher than another\n",
    "\n",
    "Pairing is particular important when there are different types of examples in the data (e.g., harder vs. easier)\n",
    "\n",
    "Compared to BT:\n",
    "\n",
    "- Mean aggregation is not robust to outliers\n",
    "- Mean and median aggregation do not take into account order of scores\n",
    "- BT is the paired variant of median (while median is the outlier-resistant version of mean)\n",
    "- BT relates to Fischer's sign test for statistical significance\n",
    "\n",
    "BT decision simply translates to computing the median of metric scores of model A and B: $BT = Median(\\mathcal{M}(A) - \\mathcal{B})$\n",
    "\n",
    "If $BT > 0$, then A is better than B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e4c96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/BT_discrepancy.png\" width=\"1000\" alt='BT_discrepancy'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ae525",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-2/BT_benchmark.png\" width=\"1000\" alt='BT_benchmark'/>\n",
    "</div>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
