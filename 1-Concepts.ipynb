{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fad4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why are you here?\n",
    "\n",
    "According to submitted Google form data, you gave the following reasons\n",
    "\n",
    "\n",
    "![google_form_true](Images/Lecture-1/google_form_true.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05785134",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## All you liars!\n",
    "\n",
    "My predictions were **wrong** since I expected 100% respondance rate for ``CFUs (Yay!!!)`` answer!!\n",
    "\n",
    "\n",
    "![SegmentLocal](Images/Lecture-1/google_form_liar.gif \"segment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775a5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anyway.. Why are you here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2bcbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Do you often struggle to reproduce a baseline for your work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bb5f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Do you get different results everytime you run your experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b6ed2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Is your work often criticized for the methodology?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf6fad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Are you often confused about what kind of experiment to carry out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f6f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Are you looking for a sound motivation to explain that your 1-point F1-score improvement is a new SOTA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49c29f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Are you looking for some ways to improve your spaghetti code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993e95e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Are you already tired of this listing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0372cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**$\\rightarrow$ This course might shed some light on these topics and help you!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabd5d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A quick Legend\n",
    "\n",
    "[‚ùì] $\\rightarrow$ This is a question for you (the audience)\n",
    "\n",
    "[‚ùó] $\\rightarrow$ A very important information to remember!\n",
    "\n",
    "[üí¨] $\\rightarrow$ An anecdote will be told\n",
    "\n",
    "[XX et al., YEAR] $\\rightarrow$ A reference (the bibliography is at the end of the presentation) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4be403",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivational Intro\n",
    "\n",
    "Let's consider a general, relatable, researcher. We name this researcher as **FR**.\n",
    "\n",
    "FR has just started their PhD and is particularly entusiastic: a lot of ideas, stuff to read, things to try.\n",
    "\n",
    "Intuitively, **FR has no idea of what comes next..**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1864f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose now that FR has a little, yet interesting, idea about their favourite research domain.\n",
    "\n",
    "[‚ùì] What does FR do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fc52b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Scientific Method [ü§ì]\n",
    "\n",
    "- Specify an hypothesis\n",
    "- Running some experiments\n",
    "- Analyze the results\n",
    "- Draw some conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913b711",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Results \n",
    "\n",
    "[‚ùó] Results have to be **repeatable** and **reproducible** to be valid and reliable $\\rightarrow$ **consistent** results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686fa84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A blind walk in the research forest\n",
    "\n",
    "FR remembers this stuff from their past studies and considers this methodology fairly standard and easy to follow.\n",
    "\n",
    "FR develops their idea, starts thinking about the experimental setup and what to code...\n",
    "\n",
    "However..some wild issues appear!\n",
    "\n",
    "![SegmentLocal](Images/Lecture-1/oh-my-god-omg.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc1935",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### w/ Data\n",
    "\n",
    "- Some interesting datasets for their study are **not available**... (*private only or premium subscription required*)\n",
    "- The available datasets are not **well-documented or maintained** (*good luck!*)\n",
    "- FR can't find the **datasheet** of the dataset (*err.. is that an insult? [üêâ]*)\n",
    "- Some of the datasets have significant **flaws/biases** **$\\leftarrow$ to be continued...**\n",
    "- Can't reproduce the same dataset by following the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50712b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### w/ Competitor models\n",
    "\n",
    "- The code of some competitor models is not available (*error 404*)\n",
    "- The code of some competitor models is outdated, horrifying, full of errors/bugs and incomplete (*pick any combination*)\n",
    "- The code of some competitor models is not exactly what you inferred after reading the paper (*tricksters*)\n",
    "- Missing hyper-parameter specs, training configuration\n",
    "- FR finally managed to run/re-implement the code for competitor models but can't reproduce the same results (*bugs?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211794e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### w/ Experimental setup\n",
    "\n",
    "- Improper use of statistical analysis\n",
    "- Over-claiming of results\n",
    "- Unfair comparison with other models, methods, etc..\n",
    "- Wrong experiments (not aligned with FR thesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a94fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A well-known issue!\n",
    "\n",
    "According to [Baker et al., 2016]:\n",
    "\n",
    "\"*More than 70% of researchers failed in their attempt to reproduce another researcher's experiments, and over 50% failed to reproduce one of their own experiments*''\n",
    "\n",
    "We observe a trend where research (in computer science) is a frenetic race to publications, often leading to significant flaws in proposed work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae53eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Moreover...\n",
    "\n",
    "\"*There's a bias in the field towards publishing positive results (rather than negative ones). Indeed, the evidence threshold for publishing a new positive finding is much lower than that for invalidating a previous finding.*''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54672320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some observations\n",
    "\n",
    "There's a growing interest in improving reproducibility across scientific disciplines: guidelines, recommendations [McNutt, 2014].\n",
    "\n",
    "#### The Open Science movement\n",
    "\n",
    "\"*Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks*'' [Vicente-Saez & Martinez-Fuentes, 2018]\n",
    "\n",
    "$\\rightarrow$ code, data, scientific communications, research artifacts should be made publicly available [Sonnenburg et al., 2007]\n",
    "\n",
    "#### Stodden's Best Practices Wiki\n",
    "\n",
    "*It's now completely acceptable for a researcher to say - here is what I did, and here are my results. [...] The presentation's core isn't about the struggle to root our error but is instead a sales pitch: an enthusiastic presentation of ideas and a breezy demo of an implementation* [Donoho et al., 2009]\n",
    "\n",
    "- Make your data, code, supplementary material and other artifacts publicly available\n",
    "- Provide version control for data and code\n",
    "- Make sure you cite original authors if using external data, code, etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c8faf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### New SOTA?\n",
    "\n",
    "We always want to achieve new SOTA: propose solutions that lead to higher metric scores.\n",
    "\n",
    "However, it is hard to assert if the aspect of a model claimed to have improved its performance is indeed the factor leading to the higher score\n",
    "\n",
    "$\\rightarrow$ A few studies show new proposed methods are often not better than previous implementations after a more thorough hyper-parameter search [Lucic et al., 2018; Melis et al., 2017] or initialization [Bouthillier et al., 2019; Henderson et al., 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b71a76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Documentation\n",
    "\n",
    "Raff, 2019 showed that 63.5% of manuscripts out of 255 were successfully reproduced given proper documentation.\n",
    "\n",
    "Strikingly, this study found that when authors provided assistance, 85% of results were reproduced (compared to 4% when authors didn't respond)\n",
    "\n",
    "$\\rightarrow$ Selection bias\n",
    "\n",
    "$\\rightarrow$ Reporting problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54378e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Statistical Analysis\n",
    "\n",
    "As opposed to most scientific disciplines, statistical analysis is seldom conducted in machine learning-based research [Forde & Paganini, 2019; Henderson et al., 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099e62e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reproducible Research [‚ùó]\n",
    "\n",
    "![reproducible_research](Images/Lecture-1/reproducible_research.png)\n",
    "\n",
    "- <u>**Reproducible**</u>: re-doing an experiment using the **same data** and **same analytical tools**\n",
    "- **Replicable**: considers **different data** (presumably, from the same distribution)\n",
    "- <u>**Robust**</u>: assumes the **same data** but **different analysis** (e.g., code re-implementation, different hardware, different architecture, etc..)\n",
    "- **Generalisable**: leads to the **same conclusions** despite considering **different data** and **different analytical tools**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88168be9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we going to cover\n",
    "\n",
    "- Setting up a correct and coherent experimental setup\n",
    "- Define and provide a reproducible and robust implementation\n",
    "\n",
    "### What does it mean?\n",
    "\n",
    "- Contextualize your contribution\n",
    "- Data understanding\n",
    "- Model experimenting\n",
    "- Model evaluation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f489f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1 - The data\n",
    "\n",
    "- Data selection\n",
    "- Understand your data\n",
    "- Data size and distribution\n",
    "\n",
    "### Data selection\n",
    "\n",
    "You think about the fanciest contribution of your life, but it will still count less than the data on which you carry out your experiments.\n",
    "\n",
    "- What does it contain?\n",
    "- Where was the data collected?\n",
    "- Are there any limitations/bias/spurious correlations? $\\rightarrow$ **The Tank problem** [üí¨]; **One my first data pre-processing on Aharoni et al., 2014** [üí¨].\n",
    "- **Popularity != quality** $\\rightarrow$ several popular datasets have shown to have significant limitations [Paullada et al., 2020]\n",
    "- Is the data directly available or built via code?\n",
    "\n",
    "#### Understand your data\n",
    "\n",
    "Data is your friend: you may spot relevant patterns that can guide your modeling. However, **only look at training data!**\n",
    "\n",
    "Otherwise, you might impose some biases that limit the generalization capabilities of your approach: **data leakage** [‚ùó]\n",
    "\n",
    "Domain experts are a valuable resource for understanding your data!\n",
    "\n",
    "#### Data size\n",
    "\n",
    "- Low amount of data? $\\rightarrow$ cross-validation, data-augmentation [Wong et al., 2016; Shorten and Khoshgoftaar, 2019] (**after data partitioning**), transfer learning, simple models.\n",
    "- Class imbalance [Haixian et al., 2017]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e94ffa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset collection\n",
    "\n",
    "According to [Paullada et al., 2021], a wide majority of datasets and related data collection processes have major flaws.\n",
    "\n",
    "- Trend on collecting large-scale datasets (quantity over quality) $\\rightarrow$ *''If it is available to us, we ingest it\"*\n",
    "- No clear understanding what these datasets/benchmarks measure.\n",
    "- Affected by spurious correlations that deep learning models use as shortcuts and overfit on them [Geirhos et al., 2020; Storks et al. 2020; Schlegel et al., 2020] $\\rightarrow$ exacerbate a dataset's utility (the hypothesis of reflecting human reasoning capabilities) $\\rightarrow$ Some solutions concerning human guidelines [Srivastava et al., 2020] and data collection recommendations [Gardner et al., 2019].\n",
    "- Improper task formulation (I/O)$\\rightarrow$ **Personal traits prediction from images** [üí¨]\n",
    "- Annotators' biases $\\rightarrow$ usually due to lack of guidelines or dataset documentation (**re-building ImageNet** [üí¨])\n",
    "- Hard to scrutinize $\\rightarrow$ abbatoir-effect [Raji et al., 2020] exacerbating data quality and hiding ethical issues\n",
    "- Entirely focused on benchmarking metrics $\\rightarrow$ consumption, cost, fairness, model size, limitations, multiple gold standards, etc..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c6da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Leakage [Kapoor & Narayaan, 2022]\n",
    "\n",
    "*Data leakage is a spurious relationship between the independent variables and the target variable that arises as an artifact of the data collection, sampling, or pre-processing strategy.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- Perform variable scaling/normalization using the whole data set.\n",
    "- Feature selection before data partitioning.\n",
    "- Dimensionality reduction before data partitioning.\n",
    "- Calibrate and test your model on the same dev/test set.\n",
    "- Data augmentation before data partitioning\n",
    "- Random splits with time series data or look ahead bias [Cerqueira et al., 2020; Wang & Ruf, 2022]\n",
    "- Biased data partitioning\n",
    "\n",
    "**$\\rightarrow$ make sure you define distinct validation and test splits** [‚ùó]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5e925",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Leakage Taxonomy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68f7e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Having valid results from which to draw reliable conclusions is a fundamental aspect of your experimental setting [Lones, 2023].\n",
    "\n",
    "- Evaluate models on an **appropriate** test set: no overlap with training data, representative of population $\\rightarrow$ **Images with same weather conditions** [üí¨]\n",
    "- In case of data-augmentation, perform it on training data only [Vandewiele et al., 2021]\n",
    "- Perform model evaluation multiple times: multiple data partitions (**input sensitivity**); multiple seeds (**random initialization sensitivity**) $\\rightarrow$ under/over-estimation of model's performance.\n",
    "- Cross-validation (stratified in case of class imbalance) $\\rightarrow$ keep a separate test set for final evaluation\n",
    "- Pick correct evaluation metrics: avoid accuracy with imbalanced datasets! [Haixian et al., 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127923c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Comparison\n",
    "\n",
    "*Current SOTA achieves 94% of accuracy, while our model achieve 95.2%* $\\rightarrow$ new SOTA! \n",
    "\n",
    "What could have gone differently? [‚ùì]\n",
    "\n",
    "- Training/evaluation on different data partitions\n",
    "- Different training dataset\n",
    "- Different training regularization\n",
    "- Mismatch in hyper-parameter optimization $\\rightarrow$ I know you have calibrated your model only!\n",
    "\n",
    "What to do?\n",
    "\n",
    "- Re-implement all models or use available code (good luck!)\n",
    "- Perform a fair hyper-parameter calibration\n",
    "- Define an exstensive experimental setup: multiple model evaluations\n",
    "- Statistical tests **$\\leftarrow$ to be continued...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2a065",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Tests\n",
    "\n",
    "Should we use statistical analysis to compare models? How to do so?\n",
    "\n",
    "**[Pairwise comparison]** Student's T test (if normally distributed), Mann-Whitneys's U test (more general) [Raschka, 2020; Carrasco et al, 2020]\n",
    "\n",
    "**[Multiple comparisons]** Multiple pairwise comparisons can lead to overly-optimistic interpretations of significance (using the test set multiple times) $\\rightarrow$ **multiplicity effect* $\\rightarrow$ **data dredging or p-hacking** [Head et al., 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a7589",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Benchmarks\n",
    "\n",
    "In some domains, it is a standard approach to use benchmark datasets to evaluate new models\n",
    "\n",
    "### Major drawback\n",
    "\n",
    "It becomes increasingly likely that the best model just happens to over-fit the test set, and doesn't necessarily generalize any better than the other models.  **The Persuasive Essays dataset** [üí¨]\n",
    "\n",
    "$\\rightarrow$ Don't assume that a small increase in performance is a significant and general contribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519657e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reporting results\n",
    "\n",
    "To effectively contyribute to knowledge, you need to provide a complete picture of your work, covering both what worked and what didn't. $\\rightarrow$ Trade-offs are common.\n",
    "\n",
    "### Transparency\n",
    "\n",
    "Share your data, code, results in an accessible way. \n",
    "\n",
    "- It adds confidence to your work\n",
    "- It allows easier and fair comparison\n",
    "- Enforces you to act thoughtfully and carefully: document your steps, write clean code, fill checklists\n",
    "\n",
    "### Rigorous evaluation\n",
    "\n",
    "A better and rigorous model evaluation and comparison is done by considering multiple datasets and evaluation metrics [Blagec et al., 2020].\n",
    "\n",
    "- Gives a more complete picture of model's performance\n",
    "- Different metrics give different perspectives $\\rightarrow$ increased transparency\n",
    "- In some domains it is very important to which errors does your model make\n",
    "\n",
    "### Over-statements\n",
    "\n",
    "A common mistake is to make general statements that are not supported by the data used to train and evaluate models.\n",
    "\n",
    "- If your model does really well on one dataset, it doesn't necessarily mean that it will do well on other datasets.\n",
    "- There's always a limit of what can be inferred from an experimental study $\\rightarrow$ sampling error/bias, datasets overlap, data quality.\n",
    "\n",
    "### Statistical significance shotgun\n",
    "\n",
    "Statistical tests are error-prone: some may underestimate significance while some others may overestimate it.\n",
    "\n",
    "$\\rightarrow$ A positive test doesn't always indicate that something is significant, and a negative test doesn't necessarily mean that something isn't significant.\n",
    "\n",
    "#### Threshold selection and abuse\n",
    "\n",
    "Statisticians are increasingly arguing that it is better not to use thresholds and just report p-values for interpretation.\n",
    "\n",
    "To give a better indication of whether something is important, we can measure **effect size** [Betensky, 2019, Benavoli et al., 2017].\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Metric reporting is complementary to model inspection as the former doesn't give insight into what the model has actually learnt.\n",
    "\n",
    "$\\rightarrow$ We don't want to just improve metric performance, but rather generate knowledge and understanding to share with the research community. [‚ùó]\n",
    "\n",
    "Some insights could be gathered through (if your model is not explainable):\n",
    "\n",
    "- Human evaluation\n",
    "- Explainable AI tools [Li et al., 2020; Angelov et al., 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f6eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
