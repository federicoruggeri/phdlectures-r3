{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48d0c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It's coding time!\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-3/programming_skills.png\" width=\"1800\" alt='programming_skills'/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6213d8f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-3/framework_knowledge.png\" width=\"2200\" alt='framework_knowledge'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea3e54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need this lecture?\n",
    "\n",
    "Whether you like it or not, experimental setting might require you to do some **coding stuff**.\n",
    "\n",
    "Coding translates to: \n",
    "\n",
    "1. Transparency (*don't you dare do some cheap tricks!*)\n",
    "2. Correctness (*your code should reflect your paper statements*) \n",
    "3. **Readability** (*please, don't make this a nightmare*)\n",
    "4. **Efficiency** (*time is money*)\n",
    "5. **Maintainability** (*I'm sure you'll re-use this code*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda74f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We should have an idea about [1-2] from past lectures!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc969f07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We mainly focus on [4] in this lecture, while we provide some tips & tricks concerning [3, 5] in the next lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee19f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we going to cover?\n",
    "\n",
    "- Dataset encoding and pre-processing pipeline\n",
    "- Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1295fc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681678be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608e7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Install if needed\n",
    "absl_py==1.4.0\n",
    "keras==2.9.0\n",
    "matplotlib==3.5.3\n",
    "memory_profiler==0.61.0\n",
    "numpy==1.21.6\n",
    "pandas==1.3.5\n",
    "scikit_learn==1.0.2\n",
    "tensorflow_gpu==2.9.0\n",
    "torch==1.13.1\n",
    "torchdata==0.5.1\n",
    "torchtext==0.14.1\n",
    "tqdm==4.64.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28683fad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### How did you do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceb2f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pipreqs\n",
    "\n",
    "pipreqs /path/to/project-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56418884",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Working environment\n",
    "\n",
    "In this lecture, we are going to alternate between jupyter notebook and Pycharm code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd51ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Why?\n",
    "\n",
    "- Coding on Jupyter is hellish (*don't do it!*)\n",
    "- I suck with notebooks and I wasn't able to even run a script\n",
    "- **Debugging** (*you know what I'm talking about, don't you?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da9af8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15582ad6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For simplicity, we are going to consider a NLP classification task on Argument Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841e724",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "We are going to consider a medium-size dataset to show the benefits of efficient coding $\\rightarrow$ time is precious!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8acb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### IBM2015 text dataset\n",
    "\n",
    "- Dataset for Argument Mining: we use for argument sentence detection (binary classification task)\n",
    "- ~82k sentences from Wikipedia articles\n",
    "- 58 controversial topics (e.g., violent videogames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2238591",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "First of all, we need to load our dataset\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/ibm2015_loader.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbff441",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class IBM2015Loader:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            load_path: AnyStr\n",
    "    ):\n",
    "        self.load_path = load_path\n",
    "\n",
    "    def load(\n",
    "            self\n",
    "    ) -> [pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        df = pd.read_csv(self.load_path)\n",
    "        train_test_df = df[df['Data-set'] == 'train and test']\n",
    "        val_df = df[df['Data-set'] == 'held-out']\n",
    "\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8)\n",
    "        train_indexes, test_indexes = list(splitter.split(X=train_test_df['Sentence'].values,\n",
    "                                                          y=train_test_df['Label'].values,\n",
    "                                                          groups=train_test_df['Topic id'].values))[0]\n",
    "        train_df = train_test_df.iloc[train_indexes]\n",
    "        test_df = train_test_df.iloc[test_indexes]\n",
    "\n",
    "        return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba93dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@evaluate_time\n",
    "def load_ibm2015_dataset(\n",
    "        samples_amount: int = -1\n",
    "):\n",
    "    # Setting\n",
    "    this_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    base_dir = os.path.normpath(os.path.join(this_path,\n",
    "                                             os.pardir,\n",
    "                                             os.pardir,\n",
    "                                             os.pardir))\n",
    "    log_dir = os.path.join(base_dir, 'logs')\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    Logger.set_log_path(name='logger',\n",
    "                        log_path=log_dir)\n",
    "    logger = Logger.get_logger(__name__)\n",
    "\n",
    "    load_path = os.path.normpath(os.path.join(base_dir,\n",
    "                                              'data',\n",
    "                                              'lecture_three',\n",
    "                                              'dataset.csv'))\n",
    "    logger.info(f'Attempting to load dataset from path: {load_path}')\n",
    "\n",
    "    # Actually loading the data\n",
    "    loader = IBM2015Loader(load_path=load_path)\n",
    "    train_df, val_df, test_df = loader.load()\n",
    "\n",
    "    if samples_amount > 0:\n",
    "        logger.info(f'Samples amount given: {samples_amount} -- Taking a slice of retrieved datasets...')\n",
    "        train_df = train_df[:samples_amount]\n",
    "        val_df = val_df[:samples_amount]\n",
    "        test_df = test_df[:samples_amount]\n",
    "\n",
    "    logger.info(f'''Loaded data: \n",
    "                Train: {train_df.shape}\n",
    "                Val: {val_df.shape}\n",
    "                Test: {test_df.shape}\n",
    "                ''')\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f3964",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset encoding and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ef452",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "Tensorflow has a good support for efficiently handling **data streams** via [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969ba49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What are we going to see\n",
    "\n",
    "- Naive pipeline\n",
    "- tf.data.Dataset.from_generator pipeline\n",
    "- tf.data.Dataset.from_generator w/ tf.py_function pipeline\n",
    "- tf.data.Dataset.from_slices pipeline\n",
    "- tf.data.TFRecordDataset\n",
    "- tf.data.TFRecordDataset w/ multi-processing\n",
    "- [**bonus**] tf.data.Dataset w/ checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781dc41d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3f8a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def __init__(\n",
    "            self\n",
    "    ):\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "    def setup(\n",
    "            self,\n",
    "            train_df: pd.DataFrame\n",
    "    ):\n",
    "        # ...\n",
    "\n",
    "    def preprocess_text(\n",
    "            self,\n",
    "            text: str\n",
    "    ) -> str:\n",
    "        # ...\n",
    "\n",
    "    def parse_inputs(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "    ) -> [np.ndarray, np.ndarray]:\n",
    "        # ...\n",
    "\n",
    "    def get_steps(\n",
    "            self,\n",
    "            data: np.ndarray\n",
    "    ) -> int:\n",
    "        # ...\n",
    "\n",
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False\n",
    "    ) -> Iterator:\n",
    "        # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8ef90",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False\n",
    "    ) -> Iterator:\n",
    "        texts, labels = self.parse_inputs(df=df)\n",
    "\n",
    "        assert len(texts) == len(labels), f'Inconsistent number of texts and labels'\n",
    "\n",
    "        num_batches = self.get_steps(data=texts)\n",
    "        for batch_idx in range(num_batches):\n",
    "            if shuffle:\n",
    "                batch_indexes = np.random.randint(low=0, high=len(texts), size=batch_size)\n",
    "            else:\n",
    "                start_index = batch_idx * batch_size\n",
    "                end_index = min(batch_idx * batch_size + batch_size, len(texts))\n",
    "                batch_indexes = np.arange(start_index, end_index)\n",
    " \n",
    "            assert len(batch_indexes) <= batch_size\n",
    "\n",
    "            batch_texts = texts[batch_indexes]\n",
    "            text_max_length = max(list(map(lambda t: len(t), batch_texts)))\n",
    "\n",
    "            batch_texts = pad_sequences(sequences=batch_texts,\n",
    "                                        maxlen=text_max_length,\n",
    "                                        padding='post',\n",
    "                                        truncating='post')\n",
    "\n",
    "            yield batch_texts, labels[batch_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1841bc3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def setup(\n",
    "            self,\n",
    "            train_df: pd.DataFrame\n",
    "    ):\n",
    "        texts = train_df['Sentence'].values\n",
    "        texts = list(map(lambda t: self.preprocess_text(t), texts))\n",
    "        self.tokenizer.fit_on_texts(texts=texts)\n",
    "\n",
    "    def preprocess_text(\n",
    "            self,\n",
    "            text: str\n",
    "    ) -> str:\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def parse_inputs(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "    ) -> [np.ndarray, np.ndarray]:\n",
    "        texts = df['Sentence'].values\n",
    "        labels = df['Label'].values\n",
    "\n",
    "        texts = list(map(lambda t: self.preprocess_text(t), texts))\n",
    "        texts = self.tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "        return np.array(texts, dtype=object), labels\n",
    "    \n",
    "    def get_steps(\n",
    "            self,\n",
    "            data: np.ndarray\n",
    "    ) -> int:\n",
    "        num_batches = int(np.ceil(len(data) / batch_size))\n",
    "        return num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181980ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/tf_naive_pipeline.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da590c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.data.Dataset\n",
    "\n",
    "How does tf.data.Dataset work first of all?\n",
    "\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-3/za-warudo.gif\" width=\"1800\" alt='JOJO_ZAWARUDO'/>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2630f30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simply put, tf.data.Dataset defines a **stream** of data\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-3/tf.data-simple-pipeline.png\" width=\"2200\" alt='tf.data'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dd78f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, we can\n",
    "\n",
    "- Load data (from generators, files, multiple inputs, arrays, etc...)\n",
    "- Transform data (tf.data.Dataset.map)\n",
    "- Shuffle data (tf.data.Dataset.shuffle)\n",
    "- Pre-load data (tf.data.Dataset.prefetch)\n",
    "- Interleave I/O and GPU operations (tf.data.Dataset.interleave)\n",
    "- Batch data (tf.data.Dataset.batch, tf.data.Dataset.padded_batch)\n",
    "- Split data into multiple workers (tf.data.Dataset.shard)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b52b48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.data.Dataset.from_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cdf85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # Note: the tf.data.Dataset.from_generator and self._make_iterator must be executed by the same python process!\n",
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False,\n",
    "            prefetch: bool = False,\n",
    "    ):\n",
    "        data_generator = partial(self.light_iterator, df=df)\n",
    "        data = tf.data.Dataset.from_generator(generator=data_generator,\n",
    "                                              output_signature=(\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                  tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "                                              ))\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        data = data.map(map_func=self.parse_inputs,\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    " \n",
    "        data = data.padded_batch(batch_size=batch_size,\n",
    "                                 padded_shapes=([None], []))\n",
    "\n",
    "        if prefetch:\n",
    "            data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05845326",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def __init__(\n",
    "            self\n",
    "    ):\n",
    "        self.tokenizer = TextVectorization()\n",
    "\n",
    "    def setup(\n",
    "            self,\n",
    "            train_df: pd.DataFrame\n",
    "    ):\n",
    "        texts = train_df['Sentence'].values\n",
    "        data = tf.data.Dataset.from_tensors(texts)\n",
    "        self.tokenizer.adapt(data=data)\n",
    "\n",
    "    def parse_inputs(\n",
    "            self,\n",
    "            text: tf.Tensor,\n",
    "            label: tf.Tensor\n",
    "    ) -> [np.ndarray, np.ndarray]:\n",
    "        text = self.tokenizer(tf.expand_dims(text, 0))[0]   # expand to add 'batch' dimension\n",
    "        return text, label\n",
    "    \n",
    "    def light_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "    ) -> Iterator:\n",
    "        texts, labels = df['Sentence'].values, df['Label'].values\n",
    "\n",
    "        assert len(texts) == len(labels), f'Inconsistent number of texts and labels'\n",
    "\n",
    "        for (text, label) in zip(texts, labels):\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f827543",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/tf_data_pipeline_gen.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc603a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.data.Dataset.from_generator w/ tf.py_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40819deb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False,\n",
    "            prefetch: bool = False,\n",
    "    ):\n",
    "        data_generator = partial(self.light_iterator, df=df)\n",
    "        data = tf.data.Dataset.from_generator(generator=data_generator,\n",
    "                                              output_types=tf.int32)\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        data = data.map(map_func=lambda idx: tf.py_function(func=partial(self.parse_inputs, df=df),\n",
    "                                                            inp=[idx],\n",
    "                                                             Tout=[tf.int32, tf.int32]),\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        data = data.padded_batch(batch_size=batch_size,\n",
    "                                 padded_shapes=([None], []))\n",
    "\n",
    "        if prefetch:\n",
    "            data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f5059",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def preprocess_text(\n",
    "            self,\n",
    "            text: str\n",
    "    ) -> str:\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def parse_inputs(\n",
    "            self,\n",
    "            index: tf.Tensor,\n",
    "            df: pd.DataFrame\n",
    "    ) -> [tf.Tensor, tf.Tensor]:\n",
    "        texts = df.iloc[index.numpy()]['Sentence']\n",
    "        labels = df.iloc[index.numpy()]['Label']\n",
    "\n",
    "        texts = list(map(lambda t: self.preprocess_text(t), [texts]))\n",
    "        texts = self.tokenizer.texts_to_sequences(texts)[0]\n",
    "        return texts, labels\n",
    "\n",
    "    def get_steps(\n",
    "            self,\n",
    "            data: np.ndarray\n",
    "    ) -> int:\n",
    "        num_batches = int(np.ceil(len(data) / batch_size))\n",
    "        return num_batches\n",
    "\n",
    "    def light_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "    ) -> Iterator:\n",
    "        for idx in range(df.shape[0]):\n",
    "            yield idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d709e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/tf_data_pipeline_gen_pyfunc.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344a5da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.data.Dataset.from_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b94893",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False,\n",
    "            prefetch: bool = False,\n",
    "    ):\n",
    "        data = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        data = data.map(map_func=self.parse_inputs,\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        data = data.padded_batch(batch_size=batch_size,\n",
    "                                 padded_shapes=([None], []))\n",
    "\n",
    "        if prefetch:\n",
    "            data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca3137",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def setup(\n",
    "            self,\n",
    "            train_df: pd.DataFrame\n",
    "    ):\n",
    "        texts = train_df['Sentence'].values\n",
    "        data = tf.data.Dataset.from_tensors(texts)\n",
    "        self.tokenizer.adapt(data=data)\n",
    "\n",
    "    def parse_inputs(\n",
    "            self,\n",
    "            inputs: Dict,\n",
    "    ) -> [np.ndarray, np.ndarray]:\n",
    "\n",
    "        text = inputs['Sentence']\n",
    "        text = self.tokenizer(tf.expand_dims(text, 0))[0]   # expand to add 'batch' dimension\n",
    "\n",
    "        return text, inputs['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d6920",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### tf.data.TFRecordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f2bff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def parse_inputs(\n",
    "            self,\n",
    "            texts: np.ndarray,\n",
    "            labels: np.ndarray\n",
    "    ) -> [List[str], np.ndarray]:\n",
    "        texts = list(map(lambda t: self.preprocess_text(t), texts))\n",
    "        texts = self.tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "        return texts, labels\n",
    "    \n",
    "    def serialize_data(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            suffix: str\n",
    "    ):\n",
    "        texts, labels = df['Sentence'].values, df['Label'].values\n",
    "        texts, labels = self.parse_inputs(texts=texts, labels=labels)\n",
    "\n",
    "        Logger.get_logger(__name__).info('Serializing data...')\n",
    "        with tf.io.TFRecordWriter(self.serialization_path + f'_{suffix}') as writer:\n",
    "            with tqdm(total=len(texts)) as pbar:\n",
    "                for idx, (text, label) in enumerate(zip(texts, labels)):\n",
    "                    feature = InputFeature(token_ids=text,\n",
    "                                           label_id=label)\n",
    "                    feature_records = InputFeature.get_feature_records(feature=feature)\n",
    "                    tf_example = tf.train.Example(features=tf.train.Features(feature=feature_records))\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "                    pbar.set_description(desc=f'Serializing example {idx}/{len(texts)}')\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fd52f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class InputFeature:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            token_ids: np.ndarray,\n",
    "            label_id: int\n",
    "    ):\n",
    "        self.token_ids = token_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "    @classmethod\n",
    "    def get_mappings(\n",
    "            cls,\n",
    "    ):\n",
    "        mappings = {\n",
    "            'token_ids': tf.io.VarLenFeature(tf.int64),\n",
    "            'label_id': tf.io.FixedLenFeature([1], tf.int64),\n",
    "        }\n",
    "        return mappings\n",
    "\n",
    "    @classmethod\n",
    "    def get_feature_records(\n",
    "            cls,\n",
    "            feature\n",
    "    ):\n",
    "        features = dict()\n",
    "        features['token_ids'] = create_int_feature(feature.token_ids)\n",
    "        features['label_id'] = create_int_feature([feature.label_id])\n",
    "        return features\n",
    "\n",
    "    @classmethod\n",
    "    def get_dataset_selector(cls):\n",
    "        def _selector(record):\n",
    "            x = record['token_ids']\n",
    "            y = record['label_id']\n",
    "            return x, y\n",
    "\n",
    "        return _selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be6a12",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            suffix: str,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False,\n",
    "            prefetch: bool = False,\n",
    "    ):\n",
    "        # Serialize only if needed!\n",
    "        if not os.path.isfile(self.serialization_path + f'_{suffix}'):\n",
    "            self.serialize_data(df=df,\n",
    "                                suffix=suffix)\n",
    "\n",
    "        data = tf.data.TFRecordDataset(self.serialization_path + f'_{suffix}')\n",
    "\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        data = data.map(lambda record: self.decode_record(record,\n",
    "                                                          name_to_features=InputFeature.get_mappings()),\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        data = data.padded_batch(batch_size=batch_size,\n",
    "                                 padded_shapes=({'token_ids': [None],\n",
    "                                                 'label_id': []}))\n",
    "\n",
    "        if prefetch:\n",
    "            data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa8d27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def decode_record(\n",
    "            self,\n",
    "            record,\n",
    "            name_to_features\n",
    "    ):\n",
    "        \"\"\"\n",
    "        TPU does not support int64\n",
    "        \"\"\"\n",
    "        example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "\n",
    "        example['token_ids'] = tf.sparse.to_dense(example['token_ids'])\n",
    "        example['label_id'] = tf.reshape(example['label_id'], ())\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21aeb66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/tf_data_pipeline_pyrecord.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805059f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.data.TFRecordDataset w/ multiprocessing\n",
    "\n",
    "We can actually serialize the dataset way faster by leveraging multi-processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce69de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def split_data(\n",
    "            self,\n",
    "            data: pd.DataFrame,\n",
    "            splits=5\n",
    "    ):\n",
    "        return np.array_split(data, splits)\n",
    "\n",
    "    def convert_split_data(\n",
    "            self,\n",
    "            split_data: List[pd.DataFrame],\n",
    "            serialization_path: AnyStr,\n",
    "            n_processes: int = 4\n",
    "    ):\n",
    "        Logger.get_logger(__name__).info(f'''Serializing data...\n",
    "            Multiprocessing info:\n",
    "                - n_processes: {n_processes}\n",
    "                - splits: {len(split_data)}\n",
    "            ''')\n",
    "\n",
    "        splits = len(split_data)\n",
    "        pbar = tqdm(total=splits)\n",
    "\n",
    "        for split in range(splits):\n",
    "            pool = mp.Pool(n_processes)\n",
    "            returns = []\n",
    "\n",
    "            output_files = [f'{serialization_path}_{split}_{proc_idx}' for proc_idx in range(n_processes)]\n",
    "            output_files = [item for item in output_files if not os.path.isfile(item)]\n",
    "\n",
    "            if not len(output_files):\n",
    "                continue\n",
    "\n",
    "            proc_split_data = np.array_split(split_data[split], n_processes)\n",
    "            for proc_idx in range(min(n_processes, len(output_files))):\n",
    "                r = pool.apply_async(self.serialize_data, args=[proc_split_data[proc_idx], output_files[proc_idx]])\n",
    "                returns.append(r)\n",
    "            pool.close()\n",
    "            for r in returns:\n",
    "                r.get()\n",
    "            pool.join()\n",
    "            pbar.set_description(desc='Completed serialization processes')\n",
    "            pbar.update(1)\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09bcee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def serialize_data(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            output_file: AnyStr\n",
    "    ):\n",
    "        texts, labels = df['Sentence'].values, df['Label'].values\n",
    "        texts, labels = self.parse_inputs(texts=texts, labels=labels)\n",
    "\n",
    "        with tf.io.TFRecordWriter(output_file) as writer:\n",
    "            for idx, (text, label) in enumerate(zip(texts, labels)):\n",
    "                feature = InputFeature(token_ids=text,\n",
    "                                       label_id=label)\n",
    "                feature_records = InputFeature.get_feature_records(feature=feature)\n",
    "                tf_example = tf.train.Example(features=tf.train.Features(feature=feature_records))\n",
    "                writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46a748",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            suffix: str,\n",
    "            splits: int = 5,\n",
    "            n_processes: int = 4,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False,\n",
    "            prefetch: bool = False,\n",
    "    ):\n",
    "        # Serialize only if needed!\n",
    "        base_dir = os.path.dirname(self.serialization_path)\n",
    "        basename = os.path.basename(self.serialization_path + f'_{suffix}')\n",
    "        if not [filename for filename in os.listdir(base_dir)\n",
    "                if basename.casefold() in filename.casefold()]:\n",
    "            split_data = self.split_data(data=df, splits=splits)\n",
    "            self.convert_split_data(split_data=split_data,\n",
    "                                    serialization_path=os.path.join(base_dir, basename),\n",
    "                                    n_processes=n_processes)\n",
    "\n",
    "        data = tf.data.Dataset.list_files(file_pattern=os.path.join(base_dir, basename + '_*'))\n",
    "        data = data.interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        data = data.map(lambda record: self.decode_record(record,\n",
    "                                                          name_to_features=InputFeature.get_mappings()),\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        data = data.padded_batch(batch_size=batch_size,\n",
    "                                 padded_shapes=({'token_ids': [None],\n",
    "                                                 'label_id': []}))\n",
    "\n",
    "        if prefetch:\n",
    "            data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38ca55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/tf_data_pipeline_pyrecord_fast.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa80c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method comparison\n",
    "\n",
    "We have run all these data loading variants...but which is **better**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e853b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We inspect:\n",
    "- Timing\n",
    "- Memory usage\n",
    "\n",
    "### Script\n",
    "\n",
    "```dataset_loading/show_stats.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94001cf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [Bonus] tf.data.Dataset w/ checkpointing\n",
    "\n",
    "One cool feature of tf.data.Dataset is that an iterator 'status' can be **saved** and **re-stored**\n",
    "\n",
    "$\\rightarrow$ Quite useful if a training routine gets **interrupted** and we want to **quickly re-cover it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c0c5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # Loading\n",
    "    train_df, val_df, test_df = load_ibm2015_dataset(samples_amount=samples_amount)\n",
    "\n",
    "    # Pre-processing pipeline\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.setup(train_df=train_df)\n",
    "\n",
    "    timing_info = {}\n",
    "    memory_info = {}\n",
    "\n",
    "    train_steps = preprocessor.get_steps(data=train_df['Sentence'].values)\n",
    "    train_iterator = partial(preprocessor.make_iterator,\n",
    "                             df=train_df,\n",
    "                             batch_size=batch_size,\n",
    "                             prefetch=prefetch,\n",
    "                             shuffle=True)\n",
    "    train_iterator = train_iterator()\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=train_iterator)\n",
    "    manager = tf.train.CheckpointManager(ckpt, os.path.join(info_save_dir, 'tf_data_pipeline_slices_ckpt'), max_to_keep=2)\n",
    "\n",
    "    run_iterator(iterator=train_iterator,\n",
    "                 takes=batches_to_take)\n",
    "\n",
    "    save_path = manager.save()\n",
    "\n",
    "    run_iterator(iterator=train_iterator,\n",
    "                 takes=batches_to_take)\n",
    "\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "\n",
    "    run_iterator(iterator=train_iterator,\n",
    "                 takes=batches_to_take)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c2e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/tf_bonus_ckpt.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94346070",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Torch\n",
    "\n",
    "Similar to Tensorflow, Torch has a good support for efficiently handling data streams.\n",
    "\n",
    "Torch data handling is just like tf.data.Dataset in terms of functionalities\n",
    "\n",
    "Currently, data handling is supported via:\n",
    "   - torch ([torch.utils.data](https://pytorch.org/docs/stable/data.html))\n",
    "   - [torchdata](https://github.com/pytorch/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb46f01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What are we going to see\n",
    "\n",
    "- Naive pipeline\n",
    "- torchdata IterableWrapper\n",
    "- torchdata TFRecordLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2082dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Datasets, DataPipes and DataLoaders\n",
    "\n",
    "Pytorch defines two main objects for data handling\n",
    "\n",
    "- Dataset: wraps a dataset to define an iterator\n",
    "- DataLoader: handles a Dataset object to enable batching, multiprocessing and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e94d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# taken from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6591e5",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55072/2496238912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "# taken from https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d66c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an emerging alternative to torch.data.Dataset, ```torchdata``` offers DataPipes\n",
    "\n",
    "$\\rightarrow$ a DataPipe is like a tf.data.Dataset\n",
    "\n",
    "- We can define it from generators, iterators, files, etc..\n",
    "- We can still define our custom DataPipe alike torch.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef44dc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb7658",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            shuffle: bool = False\n",
    "    ) -> Iterator:\n",
    "        texts, labels = self.parse_inputs(df=df)\n",
    "\n",
    "        assert len(texts) == len(labels), f'Inconsistent number of texts and labels'\n",
    "\n",
    "        num_batches = self.get_steps(data=texts)\n",
    "        for batch_idx in range(num_batches):\n",
    "            if shuffle:\n",
    "                batch_indexes = np.random.randint(low=0, high=len(texts), size=batch_size)\n",
    "            else:\n",
    "                start_index = batch_idx * batch_size\n",
    "                end_index = min(batch_idx * batch_size + batch_size, len(texts))\n",
    "                batch_indexes = np.arange(start_index, end_index)\n",
    "\n",
    "            assert len(batch_indexes) <= batch_size\n",
    "\n",
    "            batch_texts = texts[batch_indexes].tolist()\n",
    "            text_max_length = max(list(map(lambda t: len(t), batch_texts)))\n",
    "\n",
    "            batch_texts = map(lambda t: t + [0] * (text_max_length - len(t)), batch_texts)\n",
    "\n",
    "            yield batch_texts, labels[batch_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a2d54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def __init__(\n",
    "            self\n",
    "    ):\n",
    "        self.tokenizer = get_tokenizer(tokenizer='basic_english')\n",
    "        self.vocab = None\n",
    "\n",
    "    def setup(\n",
    "            self,\n",
    "            train_df: pd.DataFrame\n",
    "    ):\n",
    "        texts = train_df['Sentence'].values\n",
    "        texts = map(lambda t: self.tokenizer(self.preprocess_text(t)), texts)\n",
    "        self.vocab = build_vocab_from_iterator(iterator=texts, specials=['<UNK>'])\n",
    "\n",
    "    def preprocess_text(\n",
    "            self,\n",
    "            text: str\n",
    "    ) -> str:\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def parse_inputs(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "    ) -> [np.ndarray, np.ndarray]:\n",
    "        texts = df['Sentence'].values\n",
    "        labels = df['Label'].values\n",
    "\n",
    "        texts = list(map(lambda t: self.preprocess_text(t), texts))\n",
    "        texts = list(map(lambda t: self.vocab(self.tokenizer(t)), texts))\n",
    "        return np.array(texts, dtype=object), labels\n",
    "\n",
    "    def get_steps(\n",
    "            self,\n",
    "            data: np.ndarray\n",
    "    ) -> int:\n",
    "        num_batches = int(np.ceil(len(data) / batch_size))\n",
    "        return num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419db92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/torch_naive_pipeline.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb91ab5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### torchdata IterableWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb7807",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def make_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "            batch_size: int = 32,\n",
    "            num_workers: int = 4,\n",
    "            shuffle: bool = False\n",
    "    ) -> Iterator:\n",
    "        data_generator = partial(self.light_iterator, df=df)\n",
    "        data = IterableWrapper(data_generator(), deepcopy=False)\n",
    "\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        # Required for parallel processing\n",
    "        data = data.sharding_filter()\n",
    "\n",
    "        data = data.map(fn=self.parse_inputs)\n",
    "        data = DataLoader(data,\n",
    "                          shuffle=shuffle,  # ensures the previous shuffle works\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          collate_fn=self.batch_data)\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fb014",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def parse_inputs(\n",
    "            self,\n",
    "            input_data: Tuple[str, int]\n",
    "    ) -> [List[int], int]:\n",
    "        text, label = input_data\n",
    "        text = self.preprocess_text(text=text)\n",
    "        tokens = self.vocab(self.tokenizer(text))\n",
    "        return tokens, label\n",
    "    \n",
    "    def light_iterator(\n",
    "            self,\n",
    "            df: pd.DataFrame,\n",
    "    ) -> Iterator:\n",
    "        texts, labels = df['Sentence'].values, df['Label'].values\n",
    "\n",
    "        assert len(texts) == len(labels), f'Inconsistent number of texts and labels'\n",
    "\n",
    "        for (text, label) in zip(texts, labels):\n",
    "            yield text, label\n",
    "            \n",
    "    def batch_data(\n",
    "            self,\n",
    "            input_batch\n",
    "    ):\n",
    "        texts, labels = [], []\n",
    "        for item in input_batch:\n",
    "            texts.append(torch.tensor(item[0], dtype=torch.int32))\n",
    "            labels.append(item[1])\n",
    "\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "        labels = torch.tensor(labels, dtype=torch.int32)\n",
    "        return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa6061",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/torch_datapipe_pipeline.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470180ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### torchdata TFRecordLoader\n",
    "\n",
    "A super cool feature of torchdata is that we can directly load from tfrecord data!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78625b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def batch_data(\n",
    "            self,\n",
    "            input_batch\n",
    "    ):\n",
    "        texts, labels = [], []\n",
    "        for item in input_batch:\n",
    "            texts.append(item['token_ids'])\n",
    "            labels.append(item['label_id'])\n",
    "\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "        labels = torch.tensor(labels, dtype=torch.int32)\n",
    "        return {'token_ids': texts,\n",
    "                'label_ids': labels}\n",
    "    \n",
    "    def make_iterator(\n",
    "            self,\n",
    "            suffix: str,\n",
    "            batch_size: int = 32,\n",
    "            num_workers: int = 4,\n",
    "            shuffle: bool = False\n",
    "    ) -> Iterator:\n",
    "        base_dir = os.path.dirname(self.serialization_path)\n",
    "        basename = os.path.basename(self.serialization_path + f'_{suffix}')\n",
    "        data = FileLister(root=base_dir,\n",
    "                          masks=basename + '_*')\n",
    "        data = FileOpener(data, mode='b')\n",
    "        data = TFRecordLoader(data)\n",
    "\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=100)\n",
    "\n",
    "        data = data.sharding_filter()\n",
    "        data = DataLoader(data,\n",
    "                          shuffle=shuffle,  # ensures the previous shuffle works\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers,\n",
    "                          collate_fn=self.batch_data)\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58c8ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```dataset_loading/torch_datapipe_record_pipeline.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772bb7e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39b98d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tensorflow\n",
    "\n",
    "- Offers quite **a lot of solutions** for data handling via a unified functionality: ```tf.data.Dataset```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed1093",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tensorflow graph mode is **tricky** and defining a proper tensorflow-like pipeline is non-trivial (**more on this later!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f55a38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define a simpler pipeline via **serialization** (*my preferred solution*)\n",
    "    - Define your pythonic data pipeline\n",
    "    - Serialize it\n",
    "    - Load it efficiently via tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066476e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Serialized pipeline has also the advantage of performing a time-consuming data pipeline **just once**! (*useful for multiple concurrent training runs*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ea443",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- tf.data.Dataset pipeline is very powerful and flexible but it can be tricky to find the right **pipeline ordering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f2c03",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Torch\n",
    "\n",
    "- Powerful like Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d1b89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Currently we need ```torch``` and ```torchdata```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af2b2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Compared to Tensorflow, pipeline design is **much easier** (*personal opinion*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4070e68",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Some datapipes are not so easy to grasp as in tensorflow (*personal opinion*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea24b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Supports compatibility with tensorflow serialization! (*super cool*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2955f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb7d55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What are going to cover?\n",
    "\n",
    "- [**TF, Torch**] Training example\n",
    "- [**TF, Torch**] deterministic behaviour\n",
    "- [**TF**] tf.function\n",
    "- [**TF, Torch**] dual optimizers\n",
    "- [**TF, Torch**] broadcasting\n",
    "- [**TF, Torch**] einsum notation\n",
    "- [**TF, Torch**] tf.gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160f4b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training example\n",
    "\n",
    "Let's train a simple LSTM-based model on our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba19f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4a2e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # Loading\n",
    "    train_df, val_df, test_df = load_ibm2015_dataset(samples_amount=samples_amount)\n",
    "\n",
    "    # Pre-processing pipeline\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.setup(train_df=train_df)\n",
    "\n",
    "    train_steps = preprocessor.get_steps(data=train_df['Sentence'].values,\n",
    "                                         batch_size=batch_size)\n",
    "    train_iterator = partial(preprocessor.make_iterator,\n",
    "                             df=train_df,\n",
    "                             batch_size=batch_size,\n",
    "                             prefetch=prefetch,\n",
    "                             shuffle=True)\n",
    "\n",
    "    model = MyModel(vocab_size=preprocessor.tokenizer.vocabulary_size() + 1)\n",
    "    trainer = TFTrainer(epochs=epochs)\n",
    "    training_time, memory_usage = trainer.run(model=model,\n",
    "                                              train_data_iterator=train_iterator,\n",
    "                                              steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c877bcff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class M_LSTM(tf.keras.Model):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            vocab_size,\n",
    "            lstm_weights,\n",
    "            answer_units,\n",
    "            l2_regularization=0.,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(M_LSTM, self).__init__(**kwargs)\n",
    "        self.input_embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                         output_dim=embedding_dimension,\n",
    "                                                         mask_zero=True,\n",
    "                                                         name='input_embedding')\n",
    "        # LSTM blocks\n",
    "        self.lstm_block = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_weights))\n",
    "        self.final_block = tf.keras.layers.Dense(units=answer_units,\n",
    "                                                 kernel_regularizer=tf.keras.regularizers.l2(l2_regularization))\n",
    "\n",
    "    def call(\n",
    "            self,\n",
    "            input_ids,\n",
    "            training=False\n",
    "    ):\n",
    "        # [bs, N, d']\n",
    "        input_emb = self.input_embedding(input_ids,\n",
    "                                         training=training)\n",
    "\n",
    "        # [bs, d']\n",
    "        encoded_inputs = self.lstm_block(input_emb,\n",
    "                                         training=training)\n",
    "\n",
    "        # [bs, d'']\n",
    "        answer = self.final_block(encoded_inputs,\n",
    "                                  training=training)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05681ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TFModelWrapper:\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def loss_op(\n",
    "            self,\n",
    "            x,\n",
    "            targets,\n",
    "            training=False\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train_op(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def batch_fit(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ac7d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(TFModelWrapper):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            lstm_weights: int = 64,\n",
    "            answer_units: int = 64,\n",
    "            l2_regularization: float = 0.\n",
    "    ):\n",
    "        self.model = M_LSTM(embedding_dimension=50,\n",
    "                            vocab_size=vocab_size,\n",
    "                            lstm_weights=lstm_weights,\n",
    "                            answer_units=answer_units,\n",
    "                            l2_regularization=l2_regularization)\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    def loss_op(\n",
    "            self,\n",
    "            x,\n",
    "            targets,\n",
    "            training=False\n",
    "    ):\n",
    "        logits = self.model(x,\n",
    "                            training=training)\n",
    "\n",
    "        # Cross entropy\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                            logits=logits)\n",
    "        total_loss = tf.reduce_mean(ce)\n",
    "\n",
    "        loss_info = dict()\n",
    "        loss_info['CE'] = total_loss\n",
    "\n",
    "        # L2 regularization\n",
    "        if self.model.losses:\n",
    "            additional_losses = tf.reduce_sum(self.model.losses)\n",
    "            total_loss += additional_losses\n",
    "            loss_info['L2'] = additional_losses\n",
    "            \n",
    "        return total_loss, loss_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22179d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def train_op(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, loss_info = self.loss_op(x=x,\n",
    "                                           targets=y,\n",
    "                                           training=True)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return loss, loss_info, grads\n",
    "\n",
    "    def batch_fit(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        loss, loss_info, grads = self.train_op(x, y)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, loss_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec54141",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/tf_training_gen_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd6951",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42a008",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Loading\n",
    "    train_df, val_df, test_df = load_ibm2015_dataset(samples_amount=samples_amount)\n",
    "\n",
    "    # Pre-processing pipeline\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.setup(train_df=train_df)\n",
    "\n",
    "    train_steps = preprocessor.get_steps(data=train_df['Sentence'].values,\n",
    "                                        batch_size=batch_size)\n",
    "    train_iterator = partial(preprocessor.make_iterator,\n",
    "                             df=train_df,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=True)\n",
    "\n",
    "    model = MyModel(vocab_size=len(preprocessor.vocab) + 1)\n",
    "    trainer = ThTrainer(epochs=epochs)\n",
    "    training_time, memory_usage = trainer.run(model=model,\n",
    "                                              train_data_iterator=train_iterator,\n",
    "                                              steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c5de7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class M_LSTM(th.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            vocab_size,\n",
    "            lstm_weights,\n",
    "            answer_units,\n",
    "    ):\n",
    "        super(M_LSTM, self).__init__()\n",
    "\n",
    "        self.input_embedding = th.nn.Embedding(num_embeddings=vocab_size,\n",
    "                                               embedding_dim=embedding_dimension)\n",
    "\n",
    "        # LSTM blocks\n",
    "        self.lstm_block = th.nn.LSTM(input_size=embedding_dimension,\n",
    "                                     hidden_size=lstm_weights,\n",
    "                                     num_layers=1,\n",
    "                                     batch_first=True,\n",
    "                                     bidirectional=True)\n",
    "        self.final_block = th.nn.Linear(in_features=lstm_weights * 2,\n",
    "                                        out_features=answer_units)\n",
    "        self.final_activation = th.nn.ReLU()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids\n",
    "    ):\n",
    "        # [bs, N, d']\n",
    "        input_emb = self.input_embedding(input_ids)\n",
    "\n",
    "        # [bs, d']\n",
    "        _, (h_n, c_n) = self.lstm_block(input_emb)\n",
    "        encoded_inputs = th.permute(h_n, [1, 0, 2])\n",
    "        encoded_inputs = encoded_inputs.reshape(encoded_inputs.shape[0], -1)\n",
    "\n",
    "        # [bs, d'']\n",
    "        answer = self.final_block(encoded_inputs)\n",
    "        answer = self.final_activation(answer)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b330cba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class THModelWrapper:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "    ):\n",
    "        self.model = None\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def build_model(\n",
    "            self,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def loss_op(\n",
    "            self,\n",
    "            x,\n",
    "            targets\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train_op(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def batch_fit(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcf7b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(THModelWrapper):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            lstm_weights: int = 64,\n",
    "            answer_units: int = 64,\n",
    "            l2_regularization: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.build_model(vocab_size=vocab_size,\n",
    "                         lstm_weights=lstm_weights,\n",
    "                         answer_units=answer_units)\n",
    "        self.optimizer = th.optim.Adam(params=self.model.parameters(),\n",
    "                                       weight_decay=l2_regularization)\n",
    "        self.criterion = th.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def build_model(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            lstm_weights: int = 64,\n",
    "            answer_units: int = 64,\n",
    "    ):\n",
    "        self.model = M_LSTM(embedding_dimension=50,\n",
    "                            vocab_size=vocab_size,\n",
    "                            lstm_weights=lstm_weights,\n",
    "                            answer_units=answer_units)\n",
    "\n",
    "    def loss_op(\n",
    "            self,\n",
    "            x,\n",
    "            targets\n",
    "    ):\n",
    "        logits = self.model(x)\n",
    "\n",
    "        # Cross entropy\n",
    "        ce = self.criterion(logits, targets.long())\n",
    "        total_loss = ce\n",
    "\n",
    "        loss_info = dict()\n",
    "        loss_info['CE'] = ce\n",
    "\n",
    "        return total_loss, loss_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66a1eca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def train_op(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss, loss_info = self.loss_op(x=x,\n",
    "                                       targets=y)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return loss, loss_info\n",
    "\n",
    "    def batch_fit(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        return self.train_op(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9f96d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/torch_training_datapipe_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b58dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deterministic Behaviour\n",
    "\n",
    "Tensorflow allows to set a manual seed and to enable deterministic operations\n",
    "\n",
    "#### Note\n",
    "\n",
    "Depending on your Tensorflow version, some layers **may not support deterministic operations**! $\\rightarrow$ you'll get an runtime error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a30fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085f856",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Seeding\n",
    "fix_seed(seed=random_seed)\n",
    "tf_fix_seed(seed=random_seed)\n",
    "\n",
    "# Loading\n",
    "train_df, val_df, test_df = load_ibm2015_dataset(samples_amount=samples_amount)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085a825",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fix_seed(\n",
    "        seed: int\n",
    "):\n",
    "    Logger.get_logger(__name__).info(f'Fixing seed to: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "def tf_fix_seed(\n",
    "        seed: int\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72683e7a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea2331",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def th_fix_seed(\n",
    "        seed: int\n",
    "):\n",
    "    th.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa2be0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.function\n",
    "\n",
    "Tensorflow operates by building a **computation graph**, which is a data structure containing the operations required for executing the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65e4bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can wrap costly tensorflow-like functions via ```@tf.function``` decorator or by wrapping them via ```tf.function(func)```\n",
    "\n",
    "### Computation graph\n",
    "   - [**tracing**] it is executed in a pythonic-way **only one time** $\\rightarrow$ the computation graph is **built**\n",
    "   - new inputs flow through the computation graph $\\rightarrow$ the computation graph is **executed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd369f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # We need reduce_tracing=True since we have variable size input sequences! (token_ids)\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def batch_fit( \n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        loss, loss_info, grads = self.train_op(x, y)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, loss_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae805128",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tricky behaviours with computational graph (*know your enemy!*)\n",
    "\n",
    "In Tensorflow, the computational graph is built during tracing\n",
    "\n",
    "This means that we **cannot change the structure of the graph**, but just play with its pins (**inputs, outputs**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2c3bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: coefficient update\n",
    "\n",
    "Suppose, we have our model with a custom loss function\n",
    "\n",
    "The value of the loss is regulated by a scalar ```coefficient```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492e260",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class ExampleModel:\n",
    "\n",
    "    def __init__(\n",
    "            self\n",
    "    ):\n",
    "        self.coefficient = 1.0\n",
    "\n",
    "    @tf.function\n",
    "    def compute_overlap_constraint(\n",
    "            self,\n",
    "            assignment_matrix\n",
    "    ):\n",
    "        K = assignment_matrix.shape[-1]\n",
    "\n",
    "        # [bs, K, K]\n",
    "        root_intensities = tf.matmul(assignment_matrix, assignment_matrix, transpose_a=True)\n",
    "        root_intensities = root_intensities / stable_norm(root_intensities, axis=[1, 2])[:, None, None]\n",
    "\n",
    "        # [K, K]\n",
    "        eye_matrix = tf.eye(K)\n",
    "        eye_matrix = eye_matrix / stable_norm(eye_matrix)\n",
    "\n",
    "        # [bs, K, K]\n",
    "        penalty = root_intensities - eye_matrix[None, :]\n",
    "        penalty = stable_norm(penalty, axis=[1, 2])\n",
    "        return tf.reduce_mean(penalty) * self.coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175129c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_constraint(\n",
    "        assignment_matrix\n",
    "):\n",
    "    pairwise_diff = model.compute_overlap_constraint(assignment_matrix=assignment_matrix[np.newaxis, :, :])\n",
    "    return pairwise_diff.numpy()\n",
    "\n",
    "def simulate_coefficient_annealing(\n",
    "        model,\n",
    "        assignment_matrix: np.ndarray,\n",
    "        iterations: int = 100\n",
    "):\n",
    "    coefficient_values = np.linspace(0, 1.0, iterations)[::-1]\n",
    "    logger.info('Simulating coefficient annealing...')\n",
    "    values = []\n",
    "    with tqdm(total=iterations) as pbar:\n",
    "        for it in range(iterations):\n",
    "            constraint = compute_constraint(assignment_matrix=assignment_matrix)\n",
    "            values.append(constraint)\n",
    "\n",
    "            # Update coefficient\n",
    "            model.coefficient = coefficient_values[it]\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(desc=f'Simulating coefficient = {model.coefficient}')\n",
    "\n",
    "    logger.info(f'Constraint values: {values}')\n",
    "    plot_coefficients(values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a37e89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    @tf.function\n",
    "    def compute_overlap_constraint_with_coefficient(\n",
    "            self,\n",
    "            assignment_matrix,\n",
    "            coefficient\n",
    "    ):\n",
    "        K = assignment_matrix.shape[-1]\n",
    "\n",
    "        # [bs, K, K]\n",
    "        root_intensities = tf.matmul(assignment_matrix, assignment_matrix, transpose_a=True)\n",
    "        root_intensities = root_intensities / stable_norm(root_intensities, axis=[1, 2])[:, None, None]\n",
    "\n",
    "        # [K, K]\n",
    "        eye_matrix = tf.eye(K)\n",
    "        eye_matrix = eye_matrix / stable_norm(eye_matrix)\n",
    "\n",
    "        # [bs, K, K]\n",
    "        penalty = root_intensities - eye_matrix[None, :]\n",
    "        penalty = stable_norm(penalty, axis=[1, 2])\n",
    "        return tf.reduce_mean(penalty) * tf.cast(coefficient, penalty.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822cdb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_constraint_with_coefficient(\n",
    "        assignment_matrix,\n",
    "        coefficient\n",
    "):\n",
    "    pairwise_diff = model.compute_overlap_constraint_with_coefficient(\n",
    "        assignment_matrix=assignment_matrix[np.newaxis, :, :],\n",
    "        coefficient=coefficient)\n",
    "    return pairwise_diff.numpy()\n",
    "\n",
    "def simulate_coefficient_annealing_with_coefficient(\n",
    "        model,\n",
    "        assignment_matrix: np.ndarray,\n",
    "        iterations: int = 100\n",
    "):\n",
    "    coefficient_values = np.linspace(0, 1.0, iterations)[::-1]\n",
    "    logger.info('Simulating coefficient annealing...')\n",
    "    values = []\n",
    "    with tqdm(total=iterations) as pbar:\n",
    "        for it in range(iterations):\n",
    "            constraint = compute_constraint_with_coefficient(assignment_matrix=assignment_matrix,\n",
    "                                                             coefficient=model.coefficient)\n",
    "            values.append(constraint)\n",
    "\n",
    "            # Update coefficient\n",
    "            model.coefficient = coefficient_values[it]\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(desc=f'Simulating coefficient = {model.coefficient}')\n",
    "\n",
    "    logger.info(f'Constraint values: {values}')\n",
    "    plot_coefficients(values=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0508e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/tf_function_graph_mode_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc2ea5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dual Optimizers\n",
    "\n",
    "In many cases, we may need **multiple optimizers**\n",
    "\n",
    "- Different learning rates for different parameters\n",
    "- Multi-step training (GANs, Dual Lagrangian method, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6261f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b998e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(TFModelWrapper):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            lstm_weights: int = 64,\n",
    "            answer_units: int = 64,\n",
    "            l2_regularization: float = 0.\n",
    "    ):\n",
    "        self.model = M_LSTM(embedding_dimension=50,\n",
    "                            vocab_size=vocab_size,\n",
    "                            lstm_weights=lstm_weights,\n",
    "                            answer_units=answer_units,\n",
    "                            l2_regularization=l2_regularization)\n",
    "        self.optimizer_a = tf.keras.optimizers.Adam()\n",
    "        self.optimizer_b = tf.keras.optimizers.Adam(learning_rate=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130d3f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def train_op(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        # We set persistent=True to track multiple gradients within a single forward pass\n",
    "        # Without persistent=True, the second tape.gradient(...) fails\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss, loss_info = self.loss_op(x=x,\n",
    "                                           targets=y,\n",
    "                                           training=True)\n",
    "        grads_a = tape.gradient(loss, self.model.trainable_variables[:-2])\n",
    "        grads_b = tape.gradient(loss, self.model.trainable_variables[-2:])\n",
    "        return loss, loss_info, grads_a, grads_b\n",
    "    \n",
    "    # We need reduce_tracing=True since we have variable size input sequences! (token_ids)\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def batch_fit(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        loss, loss_info, grads_a, grads_b = self.train_op(x, y)\n",
    "        self.optimizer_a.apply_gradients(zip(grads_a, self.model.trainable_variables[:-2]))\n",
    "        self.optimizer_b.apply_gradients(zip(grads_b, self.model.trainable_variables[-2:]))\n",
    "        return loss, loss_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b00c48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/tf_dual_optimizers_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88c9e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558017c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(THModelWrapper):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            lstm_weights: int = 64,\n",
    "            answer_units: int = 64,\n",
    "            l2_regularization: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.build_model(vocab_size=vocab_size,\n",
    "                         lstm_weights=lstm_weights,\n",
    "                         answer_units=answer_units)\n",
    "\n",
    "        clf_layer_parameters = list(self.model.final_block.named_parameters())\n",
    "        clf_layer_parameter_names = [f'final_block.{item[0]}' for item in clf_layer_parameters]\n",
    "        clf_layer_parameters = [item[1] for item in clf_layer_parameters]\n",
    "        other_parameters = [v for k, v in self.model.named_parameters() if k not in clf_layer_parameter_names]\n",
    "\n",
    "        self.optimizer = th.optim.Adam(params=[\n",
    "            {'params': clf_layer_parameters, \"lr\": 1e-02},\n",
    "            {'params': other_parameters, \"lr\": 1e-03}\n",
    "        ],\n",
    "            weight_decay=l2_regularization)\n",
    "        self.criterion = th.nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89dfc3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/torch_different_lrs_example.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61cfef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(THModelWrapper):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            lstm_weights: int = 64,\n",
    "            answer_units: int = 64,\n",
    "            l2_regularization: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.build_model(vocab_size=vocab_size,\n",
    "                         lstm_weights=lstm_weights,\n",
    "                         answer_units=answer_units)\n",
    "\n",
    "        clf_layer_parameters = list(self.model.final_block.named_parameters())\n",
    "        clf_layer_parameter_names = [f'final_block.{item[0]}' for item in clf_layer_parameters]\n",
    "        self.clf_layer_parameters = [item[1] for item in clf_layer_parameters]\n",
    "        self.other_parameters = [v for k, v in self.model.named_parameters() if k not in clf_layer_parameter_names]\n",
    "\n",
    "        self.optimizer_a = th.optim.Adam(params=self.clf_layer_parameters)\n",
    "        self.optimizer_b = th.optim.Adam(params=self.other_parameters, weight_decay=l2_regularization)\n",
    "        self.criterion = th.nn.CrossEntropyLoss(reduction='mean')\n",
    "        \n",
    "    def train_op(\n",
    "            self,\n",
    "            x,\n",
    "            y\n",
    "    ):\n",
    "        self.optimizer_a.zero_grad()\n",
    "        self.optimizer_b.zero_grad()\n",
    "\n",
    "        loss, loss_info = self.loss_op(x=x,\n",
    "                                       targets=y)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer_a.step()\n",
    "        self.optimizer_b.step()\n",
    "\n",
    "        return loss, loss_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3710d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/torch_dual_optimizers_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8b2a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Broadcasting\n",
    "\n",
    "When working with tensors, broadcasting is the **101** of Tensorflow\n",
    "\n",
    "We really need to understand broadcasting since\n",
    "- It **eases** our scripting life\n",
    "- It may be **tricky** to spot some **unwanted behaviours** caused by broadcasting (*a.k.a. bugs/features*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c30cf5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142d160",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ptk_overlap_constraint(pooling_matrix):\n",
    "    # pooling_matrix is [bs, N, K]\n",
    "    K = pooling_matrix.shape[-1]\n",
    "\n",
    "    # [bs, K, N]\n",
    "    pooling_matrix = tf.transpose(pooling_matrix, [0, 2, 1])\n",
    "\n",
    "    # [bs, K, 1, N] - [bs, 1, K, N]\n",
    "    # [bs, K, K]\n",
    "    penalty = tf.reduce_sum(tf.abs(pooling_matrix[:, :, None, :] - pooling_matrix[:, None, :, :]), axis=-1)\n",
    "    penalty = tf.nn.relu(1.0 - penalty)\n",
    "\n",
    "    diag_mask = tf.ones_like(penalty) - tf.eye(penalty.shape[-1], dtype=tf.float32)[None, :, :]\n",
    "    penalty *= diag_mask\n",
    "\n",
    "    # [bs,]\n",
    "    denominator = K ** 2 - K\n",
    "    denominator = tf.maximum(tf.cast(denominator, tf.float32), 1.0)\n",
    "\n",
    "    penalty = tf.reduce_sum(penalty, axis=(-1, -2)) / denominator\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b098bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/tf_broadcasting_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a0548",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1996ae8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ptk_overlap_constraint(\n",
    "        pooling_matrix\n",
    "):\n",
    "    pooling_matrix = th.tensor(pooling_matrix)\n",
    "\n",
    "    # pooling_matrix is [bs, N, K]\n",
    "    K = pooling_matrix.shape[-1]\n",
    "\n",
    "    # [bs, K, N]\n",
    "    pooling_matrix = th.permute(pooling_matrix, [0, 2, 1])\n",
    "\n",
    "    # [bs, K, 1, N] - [bs, 1, K, N]\n",
    "    # [bs, K, K]\n",
    "    penalty = th.sum(th.abs(pooling_matrix[:, :, None, :] - pooling_matrix[:, None, :, :]), dim=-1)\n",
    "    penalty = th.relu(1.0 - penalty)\n",
    "\n",
    "    diag_mask = th.ones_like(penalty) - th.eye(penalty.shape[-1], dtype=th.float32)[None, :, :]\n",
    "    penalty *= diag_mask\n",
    "\n",
    "    # [bs,]\n",
    "    denominator = K ** 2 - K\n",
    "    denominator = th.tensor(np.maximum(denominator, 1.0))\n",
    "\n",
    "    penalty = th.sum(penalty, dim=(-1, -2)) / denominator\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b483ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/torch_broadcasting_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3de90c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Einsum notation\n",
    "\n",
    "The Einstein summation convention is the **ultimate generalization of products** such as matrix multiplication to multiple dimensions. $\\rightarrow$ [source](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a460e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### [A simple example](https://rockt.github.io/2018/04/30/einsum)\n",
    "\n",
    "Let's say we want to multiply matrix $A \\in \\mathbb{R}^{I \\times K}$ with $B \\in \\mathbb{R}^{K \\times J}$ followed by the sum of each column vector\n",
    "\n",
    "We can do\n",
    "\n",
    "$$\n",
    "    c_j = \\sum_i \\sum_k A_{ik}B_{kj} = A_{ik}B_{kj}\n",
    "$$\n",
    "\n",
    "where $c_j \\in \\mathbb{R}^{J}$ is each individual element derived by multiplying values in the column vectors $A_{i:}$ and row vectors $B_{:j}$ and summing them up\n",
    "\n",
    "The above equation can be written via Einstein notation as follows\n",
    "\n",
    "$$\n",
    "    ik,kj \\rightarrow j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b0dbd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Einsum is all you need\n",
    "\n",
    "Actually, this notation is very powerful since we can define **many operations** (*you name it!*)\n",
    "\n",
    "- Matrix transpose: ```ij->ji```\n",
    "- Sum: ```ij->```\n",
    "- Sum over axis: ```ij->j``` (axis=0), ```ij->i``` (axis=1)\n",
    "- Matrix-vector multiplication: ```ik,k->i```\n",
    "- Matrix-matrix multiplication: ```ik,kj->ij```\n",
    "- Batch matrix multiplication: ```ijk,ikl->ijl```\n",
    "- Dot product: ```i,i->```\n",
    "- Matrix dot product: ```ij,ij->```\n",
    "- Hadamard product: ```ij,ij->ij```\n",
    "- Outer product: ```i,j->ij```\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148acdf2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf5b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_transpose(x):\n",
    "    x_T = tf.transpose(x, [0, 2, 1])\n",
    "    ein_T = tf.einsum('ijk->ikj', x)\n",
    "    return x_T, ein_T\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_sum(x):\n",
    "    sum_x = tf.reduce_sum(x)\n",
    "    ein_sum = tf.einsum('ijk->', x)\n",
    "    return sum_x, ein_sum\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_axis_sum(x):\n",
    "    sum_x = tf.reduce_sum(x, axis=-1)\n",
    "    ein_sum = tf.einsum('ijk->ij', x)\n",
    "    return sum_x, ein_sum\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_mm(x):\n",
    "    mm_x = tf.matmul(x, x, transpose_a=True)\n",
    "    ein_mm = tf.einsum('ijk,ikl->ijl', tf.einsum('ijk->ikj', x), x)\n",
    "    return mm_x, ein_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722326d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/tf_einsum_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed68dab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0dc6f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_transpose(x):\n",
    "    x = th.tensor(x)\n",
    "    x_T = th.permute(x, [0, 2, 1])\n",
    "    ein_T = th.einsum('ijk->ikj', x)\n",
    "    return x_T, ein_T\n",
    "\n",
    "\n",
    "def test_sum(x):\n",
    "    x = th.tensor(x)\n",
    "    sum_x = th.sum(x)\n",
    "    ein_sum = th.einsum('ijk->', x)\n",
    "    return sum_x, ein_sum\n",
    "\n",
    "\n",
    "def test_axis_sum(x):\n",
    "    x = th.tensor(x)\n",
    "    sum_x = th.sum(x, dim=-1)\n",
    "    ein_sum = th.einsum('ijk->ij', x)\n",
    "    return sum_x, ein_sum\n",
    "\n",
    "\n",
    "def test_mm(x):\n",
    "    x = th.tensor(x)\n",
    "    mm_x = th.matmul(th.permute(x, [0, 2, 1]), x)\n",
    "    ein_mm = th.einsum('ijk,ikl->ijl', th.einsum('ijk->ikj', x), x)\n",
    "    return mm_x, ein_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77288dc1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/torch_einsum_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548200fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.gather\n",
    "\n",
    "tf.gather is an important operation since it allows to perform **multi-dimensional tensor indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcef52e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7cc781",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def supervision_loss(\n",
    "        prob_dist,\n",
    "        positive_indexes,\n",
    "        negative_indexes,\n",
    "        mask_indexes,\n",
    "        supervision_margin=0.5\n",
    "):\n",
    "    padding_amount = positive_indexes.shape[-1]\n",
    "\n",
    "    # Masking...\n",
    "\n",
    "    # Split each similarity score for a target into a separate sample\n",
    "    # similarities shape: [batch_size, memory_max_length]\n",
    "    # positive_idxs shape: [batch_size, padding_amount]\n",
    "    # gather_nd shape: [batch_size, padding_amount]\n",
    "    # pos_scores shape: [batch_size * padding_amount, 1]\n",
    "    pos_scores = tf.gather(prob_dist, positive_indexes, batch_dims=1)\n",
    "    pos_scores = tf.reshape(pos_scores, [-1, 1])\n",
    "\n",
    "    # Repeat similarity scores for non-target memories for each positive score\n",
    "    # similarities shape: [batch_size, memory_max_length]\n",
    "    # negative_idxs shape: [batch_size, padding_amount]\n",
    "    # neg_scores shape: [batch_size * padding_amount, padding_amount]\n",
    "    neg_scores = tf.gather(prob_dist, negative_indexes, batch_dims=1)\n",
    "    neg_scores = tf.tile(neg_scores, multiples=[1, padding_amount])\n",
    "    neg_scores = tf.reshape(neg_scores, [-1, padding_amount])\n",
    "\n",
    "    # Compare each single positive score with all corresponding negative scores\n",
    "    # [batch_size * padding_amount, padding_amount]\n",
    "    # [batch_size, padding_amount]\n",
    "    # [batch_size, 1]\n",
    "    # Samples without supervision are ignored by applying a zero mask (mask_res)\n",
    "    hop_supervision_loss = tf.maximum(0., supervision_margin - pos_scores + neg_scores)\n",
    "    hop_supervision_loss = hop_supervision_loss * tf.cast(mask_res, dtype=hop_supervision_loss.dtype)\n",
    "    hop_supervision_loss = tf.reshape(hop_supervision_loss, [-1, padding_amount, padding_amount])\n",
    "\n",
    "    hop_supervision_loss = tf.reduce_sum(hop_supervision_loss, axis=[1, 2])\n",
    "    \n",
    "    # Masking...\n",
    "    \n",
    "    return hop_supervision_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ac0c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_gather_nd_dim0():\n",
    "    params = tf.reshape(tf.range(15), [5, 3])\n",
    "    indexes = tf.reshape(tf.constant([[0, 0], [1, 1], [2, 2],\n",
    "                                      [0, 0], [0, 1], [4, 2]]), [2, -1, 2])\n",
    "    res = tf.gather_nd(params, indices=indexes)\n",
    "    print(f'Params: {os.linesep}{params}')\n",
    "    print(f'Indexes: {os.linesep}{indexes}')\n",
    "    print(f'Gather: {os.linesep}{res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fed62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/tf_gather_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ea1dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a752383",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_gather_dim0():\n",
    "    params = th.arange(15).reshape(5, 3)\n",
    "    indexes = th.tensor([0, 1, 2,\n",
    "                         0, 0, 4]).reshape(2, -1)\n",
    "    res = th.gather(params, dim=0, index=indexes)\n",
    "    print(f'Params: {os.linesep}{params}')\n",
    "    print(f'Indexes: {os.linesep}{indexes}')\n",
    "    print(f'Gather: {os.linesep}{res}')\n",
    "\n",
    "\n",
    "def test_gather_dim1():\n",
    "    params = th.arange(15).reshape(5, 3)\n",
    "    indexes = th.tensor([0, 1, 2,\n",
    "                         0, 0, 2]).reshape(2, -1)\n",
    "    res = th.gather(params, dim=1, index=indexes)\n",
    "    print(f'Params: {os.linesep}{params}')\n",
    "    print(f'Indexes: {os.linesep}{indexes}')\n",
    "    print(f'Gather: {os.linesep}{res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcae86",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try it out!\n",
    "\n",
    "#### Script\n",
    "\n",
    "```modeling/torch_gather_example.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b8a33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "- Data pipelines are quite useful and powerful to define efficient model training/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8c98c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model design can lead to a lot of implementation challenges to design efficient code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799de21d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### There's lot to be covered yet\n",
    "\n",
    "#### Tensorflow\n",
    "\n",
    "- Sampling\n",
    "- Distributed training\n",
    "- Tensorflow probability\n",
    "- Tensorflow addons\n",
    "- Transformers\n",
    "\n",
    "#### Torch\n",
    "\n",
    "- Sampling\n",
    "- Distributed training\n",
    "- Torch lightning\n",
    "- Transformers\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-1/jojo_tbc.gif\" width=\"600\" alt='JOJO_tbc'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 次回 (Jikai!)\n",
    "\n",
    "Knowing a library's functionalities is just the **tip of the iceberg**!\n",
    "\n",
    "We need to learn **coding best practices**\n",
    "\n",
    "- Debugging\n",
    "- Typing and type-checking\n",
    "- Model definition, training, evaluation\n",
    "- Logging\n",
    "- Unit tests\n",
    "- Controlled enviroments (e.g., Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3256c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any questions?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"Images/Lecture-1/jojo-arrivederci.gif\" width=\"1200\" alt='JOJO_arrivederci'/>\n",
    "</div>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
