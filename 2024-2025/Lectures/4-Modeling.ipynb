{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb046532",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About last time\n",
    "\n",
    "- Data is a crucial part of machine learning $\\rightarrow$ always **check the data** you use!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e7383",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Annotation paradigms encode recommendations concerning the **intended purpose** of collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081e117",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Prescriptive paradigm encourages **one shared belief**; Descriptive and perspectivist encourage **different distinct beliefs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d19c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Data collection is difficult, time-consuming and **always** biased $\\rightarrow$ in which cases biases are harmful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f532c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Trends on using as much data as possible and focus on a limited set of metrics $\\rightarrow$ high risk of **limited analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69498ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this lecture\n",
    "\n",
    "We talked a lot about data and, certainly, we did not talk enough about data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e09c0f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, there are other aspects of a machine learning pipeline where talking about reproducibility is essential: modeling, experimenting, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9b13e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What we are going to talk about\n",
    "\n",
    "- Data partitioning\n",
    "- Data leakage\n",
    "- Seeding\n",
    "- Performance comparison\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ffb0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Partitioning\n",
    "\n",
    "Data partitioning is a **crucial step** since model performance is computed on built **dev** and **test** splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509a036",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose you have some available data (e.g., you created it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa5273",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[❓] **How** should we split data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723ae90",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[❓] For **what purpose** are we splitting data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cb940",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is common practice to collect a dataset and build train, dev and test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab3a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These splits are usually built by following a specific criterion\n",
    "\n",
    "- Order of collection\n",
    "- Time order\n",
    "- Domain-specific information (e.g., different speakers, topics, sources)\n",
    "- Random !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c94b80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These splits are formally known as **standard splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c226de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This process is perfectly fine: we have the data, we follow the splits and train/evaluate our models of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301a1c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Are standard splits good?\n",
    "\n",
    "[❗] [[Gorman & Bedrick, 2019](https://aclanthology.org/P19-1267.pdf)] showed that there's a **relevant discrepancy** between standard and random splits\n",
    "\n",
    "In particular, they argue in favor of **random splits** to have better estimate of model performance on new data samples (i.e., an hypothetical real-world scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dd6f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[❗] However, [[Søgaard et al., 2021](https://aclanthology.org/2021.eacl-main.156v2.pdf)] showed that **random splits** (and other data partitioning approaches) yet provide an **over-estimate** of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b0fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/talk_about_random_splits.png\" width=\"2500\" alt='talk_about_random_splits'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cffb37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random and standard splits **under-estimate** error on new samples\n",
    "- Heuristic and adversarial splits sometime **under-estimate** error on new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f5433",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What should we do?\n",
    "\n",
    "[[Søgaard et al., 2021](https://aclanthology.org/2021.eacl-main.156v2.pdf)] propose to:\n",
    "\n",
    "1. Consider/build **multiple diverse test splits** with different biases $\\rightarrow$ better evaluation\n",
    "2. If (1) is not possible, it is better to consider **biased splits** to better approximate real-world performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc1d71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Is train-dev-test partitioning correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c77271",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[[Rob van der Goot, 2021](https://aclanthology.org/2021.emnlp-main.368.pdf)] notices\n",
    "\n",
    "- A **clear mismatch** between classical and neural models\n",
    "- **Over-estimations** on dev splits limit evaluations on test splits only $\\rightarrow$ overfitting and fast expiration of test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6ffeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Overfitting**: design decisions (e.g., hyper-parameters) $\\rightarrow$ *bias from research design* [[Hovy & Prabhumoye, 2021](https://compass.onlinelibrary.wiley.com/doi/epdf/10.1111/lnc3.12432)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823ee41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model comparison\n",
    "\n",
    "If we want to compare model A to model B, we calibrate A and B on the dev split and then evaluate them on the test split. $\\rightarrow$ model architecture search\n",
    "\n",
    "In particular, we pick the best-epoch for each model based on the dev split $\\rightarrow$ **early stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07c6f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dev split over-estimation\n",
    "\n",
    "If we calibrate on the dev split $\\rightarrow$ **overly optimistic** performance on the dev split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335761a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Thus, regarding **model picking**\n",
    "\n",
    "- If on the dev split: **over-estimation** since model picking and calibration are **both done** on the dev split\n",
    "- If on the test split: **overfitting of design decisions**, leading to faster obsolescence of the test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3d655",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What can be done?\n",
    "\n",
    "[[Rob van der Goot, 2021](https://aclanthology.org/2021.emnlp-main.368.pdf)] proposes a *tune split*\n",
    "\n",
    "1. Early stop on tune split\n",
    "2. Hyper-parameters calibration on dev split\n",
    "3. Validate results on test split\n",
    "\n",
    "This also allows a **fair comparison** with classical models since they don't use a dev split during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e36ee0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"../Images/Lecture-4/tune_split_test_performance.png\" width=\"1100\" alt='tune_split_test_performance'/> </td>\n",
    "<td> <img src=\"../Images/Lecture-4/tune_split_dev_test_performance.png\" width=\"1100\" alt='tune_split_dev_test_performance'/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc67a67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Downsides [❓]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742d673",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **More** data required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fa4fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is possible to **add** tune split to train data for final evaluation on the test split (early stop on dev)\n",
    "    - Similar to **shared tasks**: use train + dev\n",
    "    - Similar to **cross-lingual/domain** setups: source dataset dev split for model picking $\\rightarrow$ **don't use all** target languages [[Artetxe et al., 2020](https://aclanthology.org/2020.acl-main.658.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa4d36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The importance of biases\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"../Images/Lecture-4/different_time_samples.png\" width=\"1100\" alt='different_time_samples'/> </td>\n",
    "<td> <img src=\"../Images/Lecture-4/temporal_drift.png\" width=\"1100\" alt='temporal_drift'/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c6da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Leakage\n",
    "\n",
    "''*Data leakage is a spurious relationship between the independent variables and the target variable that arises as an artifact of the data collection, sampling, or pre-processing strategy.\" [[Kapoor & Narayanan, 2022](https://arxiv.org/pdf/2207.07048.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792bcbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Examples\n",
    "\n",
    "- Perform variable scaling/normalization using the whole data set.\n",
    "- Feature selection before data partitioning.\n",
    "- Dimensionality reduction before data partitioning.\n",
    "- Calibrate and test your model on the same dev/test set.\n",
    "- Data augmentation before data partitioning\n",
    "- Random splits with time series data or look ahead bias \n",
    "- Biased data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d264ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/data_leakage.png\" width=\"2000\" alt='data_leakage'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669638ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/kapoor_leakage.png\" width=\"2000\" alt='kapoor_leakage'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae6356",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [L2] Illegitimate features\n",
    "\n",
    "The model has access to features that should **not be legitimately available** for use.\n",
    "\n",
    "The judgement of whether the use of a given feature is legitimate for a modeling task requires **domain knowledge** and can be highly **problem specific**!\n",
    "\n",
    "Use **domain expertise** to decide which features are suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9f1fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [L3] Test set not drawn from the distribution of scientific interest\n",
    "\n",
    "The distribution of data on which the performance of an ML model is evaluated **differs from** the distribution of data about which the scientific claims are made.\n",
    "\n",
    "The performance of the model on the test set **does not correspond** to its performance on data drawn from the distribution of scientific interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed67508",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [L3.1] Temporal Leakage\n",
    "\n",
    "The test set should **not contain** any data from a date before the training set.\n",
    "\n",
    "If the test set contains data from before the training set, the model is built using data *''from the future\"* that it should not have access to during training.\n",
    "\n",
    "$\\rightarrow$ Avoid random splitting and check for potential **look-ahead bias** [[Cerqueira et al., 2020](https://link.springer.com/article/10.1007/s10994-020-05910-7); [Wang & Ruf, 2022](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3836631)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dd55d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [L3.2] Non-independence between train and test samples\n",
    "\n",
    "Nonindependence between train and test samples constitutes leakage, unless the scientific claim is about a distribution that has the same dependence structure.\n",
    "\n",
    "It is **quite common** that train and test samples come from the same people or unit (histopathology case [💬] [[Oner et al., 2020](https://www.medrxiv.org/content/10.1101/2020.04.23.20076406v1)])\n",
    "\n",
    "There are solutions like 'block cross-validation' [[Roberts et al., 2017](https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881); [Valavi et al., 2021](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13107)]\n",
    "\n",
    "$\\rightarrow$ Non-independence is a **hard problem** since we might not know the **underlying dependency structure** of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20a0a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [L3.3] Sampling bias in test distribution\n",
    "\n",
    "- **Spatial bias**: choosing data from a geographic location but making claims about model performance in other locations as well\n",
    "\n",
    "- **Selection bias**: choosing a non-representative subset of the dataset for evaluation (autism case [💬], pneumonia prediction [💬])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21661f91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### There are a lot of other biases (*a small memo*)\n",
    "\n",
    "- **Self-selection bias**: People with specific characteristics are more likely to agree to take part in a study than others.\n",
    "\n",
    "- **Non-response bias**: People who refuse to participate or drop out from a study systematically differ from those who take part.\n",
    "\n",
    "- **Undercoverage bias**: Some members of a population are inadequately represented in the sample.\n",
    "\n",
    "- **Survivorship bias**: Successful observations, people and objects are more likely to be represented in the sample than unsuccessful ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648c6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/Survivorship-bias.svg\" width=\"850\" alt='survivorship-bias'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13544def",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seeding\n",
    "\n",
    "Pseudo random number generation is a **critical aspect** in developing experiments.\n",
    "\n",
    "Choosing a random seed fixes the pseudo random number generation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e80083",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[❓] What processes are affected by a random seed? Have you ever fixed seeds?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173d79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Neural network initialization\n",
    "- Optimization\n",
    "- Neural network prediction\n",
    "- Data pipeline (e.g., data partitioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c23676",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[❓] How should we use random seeds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2484c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Model selection $\\rightarrow$ hyper-parameters calibration\n",
    "2. Ensemble creation\n",
    "3. Sensitivity analysis\n",
    "4. Single fixed seed $\\rightarrow$ through all model pipeline\n",
    "5. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c190510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "According to [[Bethard, 2022](https://arxiv.org/pdf/2210.13393.pdf)], based on an analysis of **85** ACL papers, points [1-3] are **safe** approaches, while points [4-5] **hide some critical risks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c8280",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, Bethard found that 48 out of 85 papers (**~56%**) follow risky approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba4895",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This trend is **unaffected by time**! $\\rightarrow$ recent papers are **still** making the same errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796e070",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model selection\n",
    "\n",
    "A random seed is just **another hyper-parameter** of the model.\n",
    "\n",
    "It is perfectly fine to find for the best random seeds like for other hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1f4e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "*''...Training multiple models with randomized initializations and use as the final model the one which achieved the best performance on the validation set''* [[Björne & Salakoski, 2018](https://aclanthology.org/W18-2311.pdf)]\n",
    "\n",
    "*''The test results are derived from the 1-best random seed on the validation set''* [[Kuncoro et al., 2020](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00345/96469/Syntactic-Structure-Distillation-Pretraining-for)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c17180",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ensemble creation\n",
    "\n",
    "Ensembling is a standard strategy to **achieve increased performance** by combining multiple models\n",
    "\n",
    "One can create an ensemble by training the same model with **different random seeds** and obtain predictions via voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395910c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*''...We ensemble five distinct models each initialized with a different random seed''* [[Nicolai et al., 2017](https://aclanthology.org/K17-2008.pdf)]\n",
    "\n",
    "*''Our model is composed of the ensemble of 8 single models. The hyperparameters and the training procedure used in each single model are the same except the random seed''* [[Yang & Wang, 2019](https://aclanthology.org/W19-4421.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2bbe1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Sensitivity Analysis\n",
    "\n",
    "Since neural networks may be **sensitive to random initialization**, one could study this aspect by considering multiple seed runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e99fa3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*''...examine the expected variance in attention-procuded weights by initializing multiple training sequences with different random seeds''* [[Wiegreffe & Pinter, 2019](https://aclanthology.org/D19-1002.pdf)]\n",
    "\n",
    "*''Our model shows a lower standard deviation on each task, which means our model is less sensitive to random seeds than other models''* [[Hua et al., 2021](https://aclanthology.org/2021.naacl-main.258.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd2af4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Single fixed seed\n",
    "\n",
    "Pick a single random seed to fix the whole model pipeline **to ensure reproducibility**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef254394",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[❓] Why is this risky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf201f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Doesn't **necessarily guarantee reproducibility** $\\rightarrow$ some libraries (e.g., Tensorflow) may **not support** seed fixing for all operations (it also depends on the **library version**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c366af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Seed fixing **implies no calibration** for such a hyper-parameter $\\rightarrow$ performance **under-estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe45553",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What should be done instead\n",
    "\n",
    "- Calibrate the random seed like any other hyper-parameter [[Dodge et al., 2020](https://arxiv.org/pdf/2002.06305.pdf)]\n",
    "- Low-resource scenario? Random search [[Bergstra & Bengio, 2012](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff8a58",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance comparison\n",
    "\n",
    "For model comparison, one should prefer **considering distributions of model performance** and not single point estimates to draw more **reliable conclusions**.\n",
    "\n",
    "[❗] However, the trend is to just consider **random seeds variations** to obtain such distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105e200",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*''We re-ran both implementations multiple times, each time only changing the seed value of the random number generator''* [[Reimers & Gurevych, 2017](https://aclanthology.org/D17-1035.pdf)]\n",
    "\n",
    "*''Indeed, the best approach is to stop reporting single-value results, and instead report the distribution of results from a range of seeds. Doing so allows for a fairer comparison across models''* [[Crane, 2018](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00018/43441/Questionable-Answers-in-Question-Answering)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ad787",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[❓] Why is this risky?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a1dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Sub-optimal models** if the goal is to compare the best possible model A from one architecture to the best possible model B from another architecture (e.g., leaderboards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19659c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. **Biased slice of family** if the goal is to compare the family of models A to the family of models B $\\rightarrow$ we should consider **all** hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40682fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What should be done instead\n",
    "\n",
    "- For (1), we should also optimize for the best possible seed $\\rightarrow$ we can still consider distributions rather than point estimates by **considering multiple test splits**.\n",
    "- For (2), we should sample different models from a family by considering **all hyper-parameters** and not just random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68f7e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Having **valid results** from which to **draw reliable** conclusions is a fundamental aspect of your experimental setting [[Lones, 2023](https://arxiv.org/pdf/2108.02497.pdf)].\n",
    "\n",
    "*Current SOTA achieves 94% of accuracy, while our model achieve 95.2%* $\\rightarrow$ **new SOTA!** [[Hooker, 1995](https://link.springer.com/article/10.1007/BF02430364)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941502cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[❓] What could have gone differently? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302a03c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training/evaluation on different **data partitions**\n",
    "- Different **training dataset**\n",
    "- Different **training regularization**\n",
    "- **Mismatch** in hyper-parameter optimization $\\rightarrow$ I know you have calibrated your model only!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb260d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e782a62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Re-implement** all models or use available code (*good luck!*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bc40f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Perform a **fair hyper-parameter** calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5633d7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluation on an **appropriate** test set: no overlap with training data, representative of population $\\rightarrow$ **Images with same weather conditions** [💬]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748056e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In case of data-augmentation, perform it on **training data only**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e59ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Perform model evaluation **multiple times** [[Blagec et al., 2020](https://arxiv.org/pdf/2008.02577.pdf)]: multiple data partitions (**input sensitivity**); multiple seeds (**random initialization sensitivity**) $\\rightarrow$ under/over-estimation of model's performance.\n",
    "    - It becomes increasingly likely that the best model just happens to **over-fit the test set**, and doesn't necessarily **generalize** any better than the other models.  **The Persuasive Essays dataset** [💬]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68852cab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-validation (**stratified** in case of class imbalance) $\\rightarrow$ keep a separate test set for final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a602e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pick **correct** evaluation metrics: different metrics give different perspectives [Wagstaff, 2012](https://icml.cc/2012/papers/298.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c598e93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a212c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Check your results $\\rightarrow$ In some domains it is very important to understand which errors does your model make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3577dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider strong simple baselines $\\rightarrow$ is there any advantage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22226e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reporting\n",
    "\n",
    "To effectively contribute to knowledge, you need to provide a **complete picture** of your work, covering both what worked and what **didn't**. \n",
    "\n",
    "$\\rightarrow$ Trade-offs are common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb19c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Over-statements\n",
    "\n",
    "A common mistake is to make **general** statements that are **not supported** by the data used to train and evaluate models.\n",
    "\n",
    "- If your model does really well on one dataset, it doesn't **necessarily mean** that it will do well on other datasets.\n",
    "- There's always a limit of what can be **inferred from** an experimental study $\\rightarrow$ sampling error/bias, datasets overlap, data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca78591",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistical significance shotgun\n",
    "\n",
    "As opposed to most scientific disciplines, statistical analysis **is seldom conducted** in machine learning-based research [[Forde & Paganini, 2019](https://arxiv.org/pdf/1904.10922.pdf); [Henderson et al., 2018](https://aaai.org/papers/11694-deep-reinforcement-learning-that-matters/)]\n",
    "\n",
    "Statistical tests are **error-prone**: some may **underestimate** significance while some others may **overestimate** it.\n",
    "\n",
    "$\\rightarrow$ A positive test **doesn't always** indicate that something is significant, and a negative test doesn't necessarily mean that something isn't significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea110e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Threshold selection and abuse\n",
    "\n",
    "Statisticians are increasingly arguing that it is better **not to** use thresholds and just report p-values for **interpretation** [[Amrhein et al., 2019](https://www.nature.com/articles/d41586-019-00857-9)]\n",
    "\n",
    "To give a better indication of whether something is important, we can measure **effect size** [[Betensky, 2019](https://www.tandfonline.com/doi/epdf/10.1080/00031305.2018.1529624?needAccess=true&role=button); [Benavoli et al., 2017](https://jmlr.org/papers/volume18/16-305/16-305.pdf)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a9d42",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Which one to use?\n",
    "\n",
    "**[Pairwise comparison]** Student's T test (if normally distributed), Mann-Whitneys's U test (more general) [[Raschka, 2020](https://arxiv.org/pdf/1811.12808.pdf); [Carrasco et al, 2020](https://www.sciencedirect.com/science/article/pii/S2210650219302639)]\n",
    "\n",
    "**[Multiple comparisons]** Multiple pairwise comparisons can lead to overly-optimistic interpretations of significance (using the test set multiple times) $\\rightarrow$ **multiplicity effect** or **data dredging** or **p-hacking** [[Head et al., 2015](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47854a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical significance\n",
    "\n",
    "A standard tool to assess that experimental results are **not coincidental**.\n",
    "\n",
    "[❓] Which statistical tool should we use? Under which conditions should we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934dc0a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[[Dror et al., 2018](https://aclanthology.org/P18-1128.pdf)] surveyed **~200** papers from ACL'17 and found that statistical significance is often **not reported** or **wrongly used**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28685f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/dror_survey.png\" width=\"300\" alt='dror_survey'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9700689",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our goal is\n",
    "\n",
    "Make sure the **difference** between two algorithms on a single comparison **is not coincidental**.\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/statistics_scooby-doo.jpg\" width=\"300\" alt='statistics_scooby-doo'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87a7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "We have two algorithms: $A$ and $B$\n",
    "\n",
    "We have a dataset $X$\n",
    "\n",
    "We have an evaluation measure $\\mathcal{M}$ $\\rightarrow$ $\\mathcal{M}(\\cdot, X)$ is its value on dataset $X$ for algorithm $\\cdot$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c4bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define the **difference** in performance between $A$ and $B$ as:\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{eq:delta}\n",
    "    \\delta(X) = \\mathcal{M}(A, X) - \\mathcal{M}(B, X)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fd062",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the stastical hypothesis testing problem is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_0: \\delta(X) \\le 0 \\\\\n",
    "    H_1: \\delta(X) > 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We want to give very low probability for $H_0$ being true in order **to reject it** and accept our desired hypothesis $H_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d565bcc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do so, we compute the p-value: the probability, under $H_0$, of obtaining a result **equal to or more extreme** than what was actually observed:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P \\left ( \\delta(Y) \\ge \\delta_{observed} | H_0 \\right ) \\quad (\\delta_{observed} \\, \\text{is derived from Eq. \\eqref{eq:delta}})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $Y$ is a random variable over possible observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8ea9b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The smaller the p-value, the higher the significance $\\rightarrow$ $H_0$ does not hold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99fdfde",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To reject $H_0$ one should define a threshold $\\alpha$, the **significance level**: reject only if p-value $< \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ad032",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Error types\n",
    "\n",
    "- Type I: $H_0$ is rejected when it is actually true\n",
    "- Type II: $H_0$ is not rejected although it should be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00740f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric vs non-parametric tests\n",
    "\n",
    "Statistical significance depends on two notions: $\\mathcal{M}$ and the distribution of $\\delta(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b84d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the distribution of $\\delta(X)$ **is known** (we call the distribution parameters $\\theta$): **parametric tests** are the way to ensure low probability of Type II error\n",
    "- Otherwise, we rely on **non-parametric** tests (less powerful but statistically sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662bbbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### How to know the distribution of $\\delta(X)$?\n",
    "\n",
    "- Shapiro-Wilk test\n",
    "- Kolmogorov-Smirnov test\n",
    "- Anderson-Darling test\n",
    "- $\\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1986dcb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric tests\n",
    "\n",
    "Typically assume the normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f0695",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Paired Student's t-test\n",
    "\n",
    "Assesses whether the **population means** of two sets of measurements differ from each other\n",
    "\n",
    "Assumes that both samples come from a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36614e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### When is it applied?\n",
    "\n",
    "On evaluation measures like accuracy, UAS (unlabeled attachment score) and LAS (labeled attachment score) (used in **structured tasks**) $\\rightarrow$ compute the mean of correct predictions per example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fa91a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Usually based on the idea of the Central Limit Theorem $\\rightarrow$ when the number of individual predictions (e.g., words in a sentence) **is large enough**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33ff7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-parametric tests\n",
    "\n",
    "There are two families\n",
    "\n",
    "- **Sampling-free**: Does **not** consider the actual values of the evaluation measures\n",
    "- **Sampling-based**: Does consider the values of the measures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553532a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Sampling-free\n",
    "\n",
    "Only considers **the number of cases** in which each of the algorithms performs better than the other\n",
    "\n",
    "Lower statistical power than sampling-based but **far less** computationally intensive\n",
    "\n",
    "- Sign test\n",
    "- McNemar's test / Cochran's Q test\n",
    "- Wilcoxon signed-rank test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8618b30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sign test\n",
    "\n",
    "**Test statistic**: number of examples for which A is better than B but **ignores the extent of the difference**!\n",
    "\n",
    "**Null hypothesis**: given a new pair of measurements $(a_i, b_i)$ then $a_i$ and $b_i$ are **equally likely** to be larger than the other\n",
    "\n",
    "**Assumptions**:\n",
    "\n",
    "- Data samples are i.i.d.\n",
    "- The differences come from a continuous distribution (e.g., accuracy metric)\n",
    "- The values are ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d2875",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### McNemar's test\n",
    "\n",
    "A **special case** of sign test for **binary classification**\n",
    "\n",
    "**Null hypothesis**: marginal probability for each outcome is the same for both A and B $\\rightarrow$ A and B are expected to make the **same proportions** of correct/incorrect predictions\n",
    "\n",
    "The Cochran's Q test generalizes for **multi-class classification** setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57df39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Wilcoxon's signed-rank test\n",
    "\n",
    "More powerful than previous methods.\n",
    "\n",
    "**Null hypothesis**: the differences follow a symmetric distribution around zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24a725",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sampling-based\n",
    "\n",
    "- Permutation/randomization test\n",
    "- Paired bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5a8dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Pitman's permutation test\n",
    "\n",
    "Computes statistical significance under all possible labellings (**permutations**) of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb61832",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   - Compute original sum of differences between A and B: $d_0$\n",
    "   - For all permutations $r$\n",
    "       - Randomly swap $\\mathcal{M}(A, x_i)$ with $\\mathcal{M}(B, x_i)$ up to $N$ times\n",
    "       - Compute sum of differences between A and B: $d_r$\n",
    "   - The p-value is the ratio of times where $d_r \\le d_0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c85119",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exponentially large** number of possible permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2c2c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In pratice, we use **approximations**: a pre-defined limited number of permutations are drawn without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f0a7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Paired bootstrap test\n",
    "\n",
    "Differs from Pitman's test by **allowing replacements**: an example from the original test data can appear more than once in a sample\n",
    "\n",
    "Uses the samples as surrogates populations for the purpose of approximating the sampling distribution of the statistic\n",
    "\n",
    "Less effective for **small** test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946e861",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which one to use?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/statistical_significance.png\" width=\"600\" alt='statistical_significance'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eecd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Open problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d47aa1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Dependent observations \n",
    "\n",
    "In many cases, samples are **not** i.i.d. (e.g., sentences from the same document)\n",
    "\n",
    "[❗] **Hard to quantify** the nature of the dependence between (test) samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691558ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cross-validation\n",
    "\n",
    "[❗] Test splits of different folds are **not independent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115bca2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A possible solution is to use $K_{Bonferroni}$ estimator [[Dror et al., 2017](https://aclanthology.org/Q17-1033.pdf)]\n",
    "  1. Calculate p-value **for each fold** separately\n",
    "  2. Perform replicability analysis for dependent datasets with $K_{Bonferroni}$\n",
    "  3. If the analysis rejects the null hypothesis in **all folds** the results should be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8417d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replicability analysis\n",
    "\n",
    "In many cases we experimentally compare a model A to another model B on several datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289f991",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, **aggregating individual statistical significance tests** over multiple datasets is error prone $\\rightarrow$ the probability of making one or more false claims is **very high**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df92e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[[Dror et al., 2017](https://aclanthology.org/Q17-1033.pdf)] propose a method for\n",
    "\n",
    "- Counting: for how many datasets does a given algorithm outperform another?\n",
    "- Identification: what are these datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e31b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Counting\n",
    "\n",
    "Recall the statistical hypothesis testing problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_0: \\delta(X) \\le 0 \\\\\n",
    "    H_1: \\delta(X) > 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3dcd60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we have $N$ datasets, we have to test for rejecting $H_0$ **$N$ times**$\\rightarrow$ it is likely to make some erroneous rejections!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc815ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, if significance level is $\\alpha = 0.05$, we have 5% chance to make an erroneous rejection. If $N = 100$, we can expect to make around 5 wrong rejections.\n",
    "\n",
    "The probability of **making at least one** erroneous rejection is $1 - (1 - 0.05)^{100} = 0.994$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ac9c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Partial conjunction test\n",
    "\n",
    "We consider different $H_0$ and $H_1$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_0^{u/N}: k < u \\\\\n",
    "    H_1^{u/N}: k \\ge u\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $k$ is the true unknown number of false null hypotheses $\\rightarrow$ number of datasets where A is **truly better** than B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81403f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This problem translates to *\"Are **at least** $u$ out of $N$ null hypotheses false?''*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5268ba0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two estimators for $k$ (i.e., for addressing counting)\n",
    "\n",
    "- Bonferroni: for **dependent** datasets\n",
    "- Fischer: for **independent** datasets (requires independence but more powerful than Bonferroni)\n",
    "\n",
    "By computing the estimator, we can estimate our $\\hat{k}$, meaning that A is better than B in **at least** $\\hat{k}$ datasets with a confidence level $1 - \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e570b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is null-hypothesis significance test the only option?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/comparison.png\" width=\"550\" alt='comparison'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7c8b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the correct way to interpret empirical observation in terms of the **superiority** of one system over another?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d36f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While $S_1$ has higher accuracy than $S_2$ in both cases, the gap is **moderate** and the datasets are of **limited size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62062d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Azer et al., 2020](https://aclanthology.org/2020.acl-main.506.pdf)] make at least **four statistically distinct hypotheses**, each supported by a **different statistical evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be82c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### H1 (null hypothesis)\n",
    "\n",
    "**Assuming** $S_1$ and $S_2$ have inherently **identical** accuracy, the probability (p-value) of making a hypothetical observation with an accuracy gap at least as large as the empirical observation (here, 3.5%) is at most 5% (making us 95% confident that the above assumption is false)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e6429",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### H2 (confidence intervals)\n",
    "\n",
    "**Assuming** $S_1$ and $S_2$ have inherently **identical** accuracy, the empirical accuracy gap (here, 3.5%) is\n",
    "larger than the maximum possible gap (confidence interval) that could hypothetically be observed with a\n",
    "probability of over 5% (making us 95% confident that the above assumption is false)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4db4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### H3 (posterior intervals)\n",
    "\n",
    "**Assume a prior belief** (a probability distribution) w.r.t. the inherent accuracy of typical systems. Given the\n",
    "empirically observed accuracies, the **probability** (posterior interval) that the inherent accuracy of S1 exceeds that of S2 **by a margin** of 1% is at least 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880412e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### H4 (Bayes factor)\n",
    "\n",
    "**Assume a prior belief** (a probability distribution) w.r.t. the inherent accuracies of typical systems. Given the empirically observed accuracies, **the odds** increase by a factor in favor of the hypothesis that the inherent accuracy of S1 exceeds that of S2 **by a margin** of 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe50497",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "H1 and H2 can be tested with **p-value-based** methods $\\rightarrow$ operate over the probability space of a test statistics ($\\delta$) over observations\n",
    "\n",
    "H3 and H4 are based on **Bayesian inference** $\\rightarrow$ operate directly over the probability space of inherent accuracy (rather than observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e6d23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Due to **time reasons**, we don't explore these methods in details. $\\rightarrow$ a dedicated course would be needed! (*I suck at statistics*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d76a56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I **recommend** further reading [[Azer et al., 2020](https://aclanthology.org/2020.acl-main.506.pdf)] as it clearly describes all possible misconceptions, misuses and statistical tools when comparing two models on some observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91503875",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/azer_survey.png\" width=\"1700\" alt='azer_survey'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e93d3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Takeaways\n",
    "\n",
    "1. p-values **do not provide probability** estimates on two systems **being different** (or equal). They can only allow you to conclude that one system **is better than** the other without taking into account **the extent** of the difference between the systems (binary thinking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ec5c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. A common **misconception** is: *\"if p-value $< 0.05$, the null hypothesis has only a 5% chance of being true''* $\\rightarrow$ this is false since we are defining p-value with the **assumption** of null hypothesis being true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f562f23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Another **misconception** is: *\"if p-value $> 0.05$, there is no difference between the two systems''* $\\rightarrow$ a large p-value only means that the null hypothesis **is consistent with observations**. It does **not** tell you anything about the **likeliness** of the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75363935",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Another misconception is: *\"a statistically significant result (p-value $< 0.05$) indicates a **notable difference** between the two systems''* $\\rightarrow$ p-value only indicates **strict superiority** and provides **no information** about the **margin of the effect** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a316fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. **Posterior intervals** generally provide a useful summary as they capture **probabilistic estimates** of the correctness of the hypothesis. For instance, we can say \"with probability 0.95, model A's accuracy is two percent higher than that of B $\\rightarrow$ A outperforms B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3ea1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sticking to widely known reporting standards\n",
    "\n",
    "[[Marie et al., 2021](https://aclanthology.org/2021.acl-long.566.pdf)] analyze **769** papers concerning machine translation and observe that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1afdfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- BLEU metric is used as a reference metric in **99%** of the papers, while there are **more than 100 metrics** that better correlate with human judgements than BLEU $\\rightarrow$ different metric choice leads to **different model rankings**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf16099",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Significance testing is **only used in 65%** of the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1e520",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Just considering 2019-2020, **40%** of the papers copied results of previous work $\\rightarrow$ it is often unclear whether copied and proper results are **comparable**! (metrics may have **parameters**)\n",
    "    - For instance, a metric may depend on the tokenizer used and the pre-processing pipeline\n",
    "    - Previous work may hide **coding tricks** [[Liao et al., 2021](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/757b505cfd34c64c85ca5b5690ee5293-Paper-round2.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae7dd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Just considering 2019-2020, **38%** of the papers claimed superiority of a particular method but used different data (e..g, pre-processing pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c87fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More in general, there are several factors that may lead to a method being perceived as superior to another: *benchmark lottery* [[Dehghani et al., 2021](https://arxiv.org/pdf/2107.07002.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82ca4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aggregating results via averaging\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/BT_example.png\" width=\"600\" alt='BT_example'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b1fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[[Peyrard et al., 2021](https://aclanthology.org/2021.acl-long.179.pdf)] argue that **averaging across samples** on the same test set for comparing two models is not a **robust** approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d8e27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "They propose the **Bradley-Terry (BT)** model to account for **sample-level pairing**: compares systems for each test instance and estimates the latent strength of systems based on **how frequently** one system scores higher than another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a89ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pairing is particular important when there are **different types of examples** in the data (e.g., harder vs. easier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819f848",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compared to BT:\n",
    "\n",
    "- Mean aggregation is **not robust** to outliers\n",
    "- Mean and median aggregation **do not take into account order of scores**\n",
    "- BT is the paired variant of median (while median is the **outlier-resistant** version of mean)\n",
    "- BT relates to Fischer's sign test for statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e329fd22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BT **decision** simply translates to computing the median of metric scores of model A and B: $BT = Median \\left ( \\mathcal{M}(A) - \\mathcal{M}(B) \\right )$\n",
    "\n",
    "If $BT > 0$, then A is **better than** B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e4c96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/BT_discrepancy.png\" width=\"1000\" alt='BT_discrepancy'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ae525",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-4/BT_benchmark.png\" width=\"1600\" alt='BT_benchmark'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a257cf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "- Data partitioning is a very delicate step that could lead to over-estimations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d19f48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Data leakage can take different forms and is quite common! $\\rightarrow$ pay attention to what you do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056aa29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random seeds should be treated as any other hyper-parameter $\\rightarrow$ evaluate on more test splits!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572b29e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Statistical significance is often abused and erroneously applied $\\rightarrow$ we really need to understand it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bba58a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any questions?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-2/jojo-arrivederci.gif\" width=\"1000\" alt='JOJO_arrivederci'/>\n",
    "</div>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
