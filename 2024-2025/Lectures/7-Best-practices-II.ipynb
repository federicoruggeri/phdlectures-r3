{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8b8a33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About last lecture\n",
    "\n",
    "- Catching bugs requires a **lot of effort**! Luckily, we have many weapons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa26ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Many best practices for debugging and code organization also **greatly increase readability**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc10a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are **several tips** concerning **code organization and structure** that we should depending on our scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43dc590",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are many more, often **problem specific** and **very hard to generalize**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a981a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's left?\n",
    "\n",
    "We did briefly overview many general aspects of coding, from debugging to coding principles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fbe62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But many of us are interested in domain-specific coding tips $\\rightarrow$ **deep learning**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc5173",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this lecture\n",
    "\n",
    "We are going to briefly overview some major coding aspects in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106200c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mainly showing examples in Pytorch and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f6856",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DL Best Practices\n",
    "\n",
    "### Modeling ([Karpathy's blog](https://karpathy.github.io/2019/04/25/recipe/))\n",
    "- Inspect data\n",
    "- Start simple\n",
    "- Overfit\n",
    "- Apply some regularizations\n",
    "- Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbf180",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scripting\n",
    "- File Organization\n",
    "- Sequential layers\n",
    "- Calling layers\n",
    "- Training and evaluation modes\n",
    "- Mixing operations\n",
    "- Detaching\n",
    "- Numerical stability\n",
    "- Shuffling data\n",
    "- Gradient clipping\n",
    "- tf.reshape/th.view vs tf.transpose/th.permute\n",
    "- Squeezing\n",
    "- Working with GPU devices\n",
    "- Freeing GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3ab05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## As always, let's first check your experience!\n",
    "\n",
    "Time for another [Google Form](https://forms.gle/qmQuLWXaygSjft3V7) (**5 mins**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734f1b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-7/qsn_dl.png\" width=\"600\" alt='Deep Learning'/>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301703c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling\n",
    "\n",
    "*Because, eventually, we all prefer a todo listing...*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe98dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intro\n",
    "\n",
    "Neural networks are **not a plug-and-play module** that is expected to work effortlessly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ba24f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead, neural networks often **fail silently** (*no exceptions!*) and can still manage to reach **some satisfying result** (*unexpected*)\n",
    "   - Leakage\n",
    "   - Data errors\n",
    "   - Wrong initialization, regularization, optimization, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613932c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What you should do (*in pills*)\n",
    "\n",
    "- **Start slow** $\\rightarrow$ don't be eager to test out your super fancy model on your ultra fancy data\n",
    "- **Check** your data attentively\n",
    "- **Start simple** $\\rightarrow$ progressively introduce complexity\n",
    "- **Hammer your model** until it is sufficiently powerful and regularized to work well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4bd12",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Become one with the data\n",
    "\n",
    "Intuitively, we **never start from modeling**, but rather we **look at data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f06c8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Understand relevant **features** for addressing the task\n",
    "- Spot **outliers** (*may be symptomatic of data collection errors*)\n",
    "- Spot **errors** (e.g., duplicates, wrong labeling)\n",
    "- Check data **distributions**\n",
    "- Identify **biases**\n",
    "- Identify possible **correct pre-processing** steps\n",
    "- Helps in **understanding** model post-training evaluation (*error analysis*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee85f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Start simple\n",
    "\n",
    "Still, we are not ready yet to plug-in our fancy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dfb07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Write a **training/evaluation skeleton**\n",
    "- Test **simple baselines** (*how far can they go?*) $\\rightarrow$ downplaying possible scripting errors\n",
    "- **Avoid** any unnecessary complexity $\\rightarrow$ pick the simplest setting possible\n",
    "- **Overfit one single batch** $\\rightarrow$ spotting errors, evaluate model capacity, evaluate data\n",
    "- Check **learning curves**\n",
    "- Inspect model **prediction dynamics** $\\rightarrow$ gives good intuition of model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014719e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfit\n",
    "\n",
    "Pick a **large enough model** that is able to overfit $\\rightarrow$ we understand that the task is *solvable* on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b15f9b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pick **existing models** (*even their simplest version*) rather than making custom ones\n",
    "- **Progressively** introduce complexity $\\rightarrow$ multiple inputs, larger inputs, etc..\n",
    "- Pay attention to employed **optimizers** $\\rightarrow$ weight decay, learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e732f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularize\n",
    "\n",
    "Once we have a big enough model, we have to **regularize it** to allow better generalization capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa76b48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Add **more data** (*if you can!*) $\\rightarrow$ data-augmentation, real data, synthetic data\n",
    "- Check **spurious** inputs\n",
    "- **Decrease** model size\n",
    "- **Decrease** batch size (*if you are using batch normalization*)\n",
    "- Dropout\n",
    "- Weight decay\n",
    "- Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8db45b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning\n",
    "\n",
    "To find a good regularized model, we may consider a **hyper-parameter calibration** routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447cbab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random search (*it usually works quite well to explore the hyper-parameter space*)\n",
    "- Bayesian optimization (e.g., Optuna)\n",
    "- **Use your brain** $\\rightarrow$ in many cases, you may need to think about the valid search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e6d48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scripting\n",
    "\n",
    "Let's see some concrete examples of advanced coding that might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f20b72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### File Organization\n",
    "\n",
    "**Split** your model into individual layers and losses to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea9b6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Enhance **re-usability** (*easier to spot errors*)\n",
    "- Enhance **readability** (*top-down view of a model*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711517e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The same applies for **nested** models, layers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad35ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# losses.py\n",
    "class CustomLoss(th.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "        \n",
    "# layers.py\n",
    "class CustomLayer(th.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ...\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ...\n",
    "        \n",
    "# models.py\n",
    "class CustomModel(th.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.layer = CustomLayer(...)\n",
    "        self.loss_op = CustomLoss(...)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a19db7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In **Tensorflow** this is particularly recommended when considering ```tf.function```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58ad34",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### tf.function covers function nesting\n",
    "\n",
    "If a function invokes other functions, you just need to decorate the top-level function with ```tf.function```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95d072",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def loss_op(self, x, y, training=False):\n",
    "        output = self.model(x, training=training)\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "    def train_op(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_op(x=x, y=y, training=True)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_fit(self, x, y):\n",
    "        loss, grads = self.train_op(x, y)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                           self.model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_evaluate(self, x, y):\n",
    "        return self.loss_op(x=x, y=y, training=False)\n",
    "        \n",
    "    @tf.function\n",
    "    def batch_predict(self, x):\n",
    "        return self.model(x, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5db125",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequential Layers\n",
    "\n",
    "In many cases, we may have to define a sequential network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c70aeb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = th.nn.Sequential(\n",
    "            th.nn.Conv2d(...),\n",
    "            th.nn.ReLU(...),\n",
    "            th.nn.BatchNorm2d(...)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e21739",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = [\n",
    "            th.nn.Conv2d(...),\n",
    "            th.nn.ReLU(...),\n",
    "            th.nn.BatchNorm2d(...)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e25c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = th.nn.Conv2d(...)(x)\n",
    "        x = th.nn.ReLU(...)(x)\n",
    "        x = th.nn.BatchNorm2d(...)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3793a29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which one do you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d1272",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### (A) is the best choice\n",
    "\n",
    "- [**TH**] Uses nn.Sequential(...) to define a sequential network $\\rightarrow$ higher efficiency, readibility\n",
    "- [**TF**] Likewise, use ```tf.keras.Sequential``` in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d02ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### (B) may give some problems with list wrapping\n",
    "\n",
    "- [**TH**] Consider using ```th.nn.ModuleList(...)``` rather than a list\n",
    "- [**TF**] It is **fine to list wrapping** multiple layers, but the **recommended way** is to use ```tf.keras.Sequential```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bfc88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### (C) is terrible!\n",
    "\n",
    "- Generates layers at **each** forward pass $\\rightarrow$ losing track of model weights to update\n",
    "- You need to define layers in the ```__init__(...)``` method so that model weights are **kept throughout the life the of the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561da159",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calling layers\n",
    "\n",
    "Both Tensorflow and Pytorch implement layers as **callable** objects\n",
    "\n",
    "**Always invoke** your layer/model as ```layer(...)``` and ```model(...)```, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac40308",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Never invoke ```call(...)``` or ```forward(...)``` explicitly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1d94a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training and Evaluation modes\n",
    "\n",
    "Torch has two model modalities: ```model.train()``` and ```model.eval()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9d6bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_iterator = data_iterator()\n",
    "model.eval()\n",
    "for step in range(steps):\n",
    "    batch = next(data_iterator)\n",
    "    batch_loss, batch_preds = model.batch_predict(*batch)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2d8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_iterator = data_iterator()\n",
    "model.eval()\n",
    "with th.no_grad():  # <--- the difference is here\n",
    "    for step in range(steps):\n",
    "        batch = next(data_iterator)\n",
    "        batch_loss, batch_preds = model.batch_predict(*batch)\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e4f77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which one do you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef5e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Both are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c58e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Model.eval()\n",
    "\n",
    "- Just **changes model execution** so that layers like Dropout, BatchNorm can execute correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d55a35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### th.no_grad()\n",
    "\n",
    "- **Disables automatic differentiation** saving up memory and time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98370e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the common case where you **don't compute** any gradient during evaluation, you can use both to **gain** some speed-up and use less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6b932",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Beware of incorrect behaviours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb984e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, train_steps, train_data_iterator,\n",
    "          val_steps, val_data_iterator):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_iterator = train_data_iterator()\n",
    "        for step in range(train_steps):\n",
    "            batch = next(epoch_train_iterator)\n",
    "            batch_loss = model.batch_fit(*batch)\n",
    "            \n",
    "        # Get loss on validation set\n",
    "        val_loss, val_metrics = evaluate(model=model,\n",
    "                                         steps=val_steps,\n",
    "                                         data_iterator=val_data_iterator)\n",
    "                \n",
    "                \n",
    "def evaluate(model, steps, data_iterator):\n",
    "    model.eval()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e247c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can you spot the **error**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45979197",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tensorflow is as **straightforward** as Pytorch\n",
    "\n",
    "You just have to remember to call your model with ```training=True|False``` for training and evaluation modes, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fefe8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    def loss_op(self, x, y, training=False):\n",
    "        output = self.model(x, training=training)  # <---\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "    def train_op(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_op(x=x, y=y, training=True)   # <---\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_fit(self, x, y):\n",
    "        loss, grads = self.train_op(x, y)\n",
    "        self.optimizer.apply_gradients(zip(grads, \n",
    "                                           self.model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def batch_evaluate(self, x, y):\n",
    "        return self.loss_op(x=x, y=y, training=False)   # <---\n",
    "        \n",
    "    @tf.function\n",
    "    def batch_predict(self, x):\n",
    "        return self.model(x, training=False)  # <---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead0ddf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixing operations\n",
    "\n",
    "Consider the following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676d073",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Numpy version\n",
    "loss = np.square(y_pred - y_true).sum()\n",
    "\n",
    "# Torch version\n",
    "loss = (y_pred - y_true).pow(2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbefa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The numpy code is **always run on the CPU**, while the torch code may also run on the GPU\n",
    "\n",
    "- **Avoid mixing** numpy and torch operations in ```forward(...)``` method since numpy operations slow down your code execution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f71b4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Detaching\n",
    "\n",
    "How to **properly** collect model outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012ea37",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(self.epochs):\n",
    "    epoch_train_iterator = train_data_iterator()\n",
    "    for step in range(steps):\n",
    "        batch = next(epoch_train_iterator)\n",
    "        batch_loss, batch_loss_info = model.batch_fit(*batch)\n",
    "        batch_loss_info = {f'train_{key}': item.detach().numy() \n",
    "                           for key, item in batch_loss_info.items()}\n",
    "        batch_loss_info['train_loss'] = batch_loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9a4ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(self.epochs):\n",
    "    epoch_train_iterator = train_data_iterator()\n",
    "    for step in range(steps):\n",
    "        batch = next(epoch_train_iterator)\n",
    "        batch_loss, batch_loss_info = model.batch_fit(*batch)\n",
    "        batch_loss_info = {f'train_{key}': item.detach().numpy()\n",
    "                           for key, item in batch_loss_info.items()}\n",
    "        # the difference is here\n",
    "        batch_loss_info['train_loss'] = batch_loss_info  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70abf95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [TH] Detaching is need!\n",
    "\n",
    "- **Removes a tensor from torch tracking** for automatic differentation\n",
    "- If you don't do that, the unnecessary recording of these tensors **slows down** your program execution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d073b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### [TF] use ```tensor.numpy()```\n",
    "\n",
    "- If your operation is **outside the tensorflow graph**, you receive a ```tf.EagerTensor``` storing numerical content (based on provided inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3d73f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical Stability\n",
    "\n",
    "Mathematical correctness of your code **doesn't necessarily translates** to correct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dc4ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# from https:/github.com/vahidk/EffectivePytorch\n",
    "def unstable_softmax(logits):\n",
    "    exp = th.exp(logits)\n",
    "    return exp / th.sum(exp)\n",
    "\n",
    "# prints [nan, 0.]\n",
    "print(unstable_softmax(th.tensor([1000., 0.])).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349293b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# from https:/github.com/vahidk/EffectivePytorch\n",
    "def unstable_softmax_cross_entropy(labels, logits):\n",
    "    logits = th.log(th.nn.softmax(logits))\n",
    "    return -th.sum(labels * logits)\n",
    "\n",
    "labels = th.tensor([0.5, 0.5])\n",
    "logits = th.tensor([1000., 0.])\n",
    "\n",
    "# prints inf\n",
    "ce = unstable_softmax_cross_entropy(labels, logits).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6e7a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shuffling data\n",
    "\n",
    "One common error is to **not appropriately shuffle data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fd180",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def light_iterator(\n",
    "        df: pd.DataFrame,\n",
    ") -> Iterator:\n",
    "    texts, labels = df.text.values, df.label.values\n",
    "    assert len(texts) == len(labels), \n",
    "    f'Inconsistent number of texts and labels'\n",
    "\n",
    "    for (text, label) in zip(texts, labels):\n",
    "        yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644d6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "from functools import partial\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "\n",
    "data_generator = partial(light_iterator, df=df)\n",
    "data = IterableWrapper(data_generator(), deepcopy=False)\n",
    "\n",
    "if shuffle:\n",
    "    data = data.shuffle(buffer_size=100)    # <---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f667d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "data_generator = partial(light_iterator, df=df)\n",
    "data = tf.data.Dataset.from_generator(generator=data_generator,\n",
    "                                      output_signature=(\n",
    "                                          tf.TensorSpec(shape=(),\n",
    "                                                        dtype=tf.string),\n",
    "                                          tf.TensorSpec(shape=(),\n",
    "                                                        dtype=tf.int64)\n",
    "                                      ))\n",
    "if shuffle:\n",
    "    data = data.shuffle(buffer_size=100)     # <---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbb8a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Beware of buffer_size\n",
    "\n",
    "These data pipeline APIs work by setting up a **buffer from which we sample** examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf2d1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If your buffer is **too small** you may just sampling class-equivalent examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412caf4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Always** pre-shuffle your data (if possible)\n",
    "- Set a ```buffer_size``` **equal to** your data size (if not too big)\n",
    "- **Inspect** your data stream for sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e52b96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient clipping\n",
    "\n",
    "In many cases, you may want to clip gradients **to increase model training stability**\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-7/gradient_clipping.png\" width=\"1300\"/>\n",
    "</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e7b33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "th.nn.utils.clip_grad_norm(model.parameters(), max_norm=10)\n",
    "\n",
    "# Tensorflow\n",
    "grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "grads, _ = tf.clip_by_global_norm(grads, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0234c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### tf.reshape/th.view vs tf.transpose/th.permute\n",
    "\n",
    "In many cases, we **may erroneously** use one operation in place of the other one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d377d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.reshape(tf.range(6), [2, 3])\n",
    "# <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
    "#  array([[0, 1, 2], [3, 4, 5]], dtype=int32)>\n",
    "\n",
    "tf.reshape(x, [3, 2])\n",
    "# <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
    "# array([[0, 1],\n",
    "#        [2, 3],\n",
    "#        [4, 5]], dtype=int32)>\n",
    "\n",
    "tf.transpose(x)\n",
    "# <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
    "# array([[0, 3],\n",
    "#        [1, 4],\n",
    "#        [2, 5]], dtype=int32)>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587f535",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Beware of reshaping!\n",
    "\n",
    "If used incorrectly, it may lead to **leakage between** batch samples! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5744b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Squeezing\n",
    "\n",
    "Squeezing is the operation of removing 1-unit dimensions in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18babcd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shape -> [2, 3, 1, 1]\n",
    "x = tf.reshape(tf.range(6), [2, 3, 1, 1])\n",
    "\n",
    "# Shape -> [2, 3]\n",
    "x = tf.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c0316",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Beware of squeezing with batched tensors\n",
    "\n",
    "One time I lost a **couple of hours** with a strange error..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16a65f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It turned out that the batching with a specific ```batch_size``` led to a batch **with a single sample**\n",
    "\n",
    "$\\rightarrow$ squeezing without specifying any dimension inherently converted my input 3D tensors to 2D!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e119168",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working with GPU devices\n",
    "\n",
    "When working with GPUs we have to carefully inspect **how** these devices are used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f52c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ this is particularly **annoying** in Tensorflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d42a72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tensorflow automatically reserves **all available memory** from the selected GPU\n",
    "- Moreover, Tensorflow also reserves some memory in **all discovered GPUs**, even if you are in a single-GPU setting (*efficiency reasons to reduce memory fragmentation*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a5453",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Enforces Tensorflow to just use the necessary amount of GPU memory\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Limiting visibility (only first gpu)\n",
    "gpu_start_index = 0\n",
    "gpu_end_index = 1\n",
    "tf.config.set_visible_devices(gpus[gpu_start_index:gpu_end_index],\n",
    "                              \"GPU\")\n",
    "\n",
    "# Setting max GPU memory (e.g., 3GB on first GPU)\n",
    "tf.set_virtual_device_configuration(gpus[0],\n",
    "                [tf.VirtualDeviceConfiguration(memory_limit=3072)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de85fd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "# Setting max GPU memory (e.g., 30% of total GPU memory)\n",
    "th.cuda.set_per_process_memory_fraction(0.3, device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5e947",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Freeing GPU memory\n",
    "\n",
    "In many cases, you may find yourself in a scenario where **multiple models** have to be trained (e.g., cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab85368",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In these cases, you may need to **efficiently release GPU usage** to avoid memory problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310777c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "from tensorflow.python.keras import backend as K\n",
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72f8d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "del model\n",
    "gc.collect()\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf6776",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Manually freeing GPU memory may not be needed\n",
    "\n",
    "Generally speaking, Tensorflow/Torch **might efficiently re-use** the previously allocated memory\n",
    "\n",
    "$\\rightarrow$ manually freeing memory might lead to **minor code execution** speed reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d770a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow has a problematic GPU memory management\n",
    "\n",
    "According to this [thread](https://github.com/tensorflow/tensorflow/issues/36465), commands like ```K.clear_session()``` **may not really work**.\n",
    "\n",
    "Instead, the recommended way is to run your train/evaluation routine in a **separate process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f59f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "process_eval = multiprocessing.Process(target=evaluate, args=(...))\n",
    "process_eval.start()\n",
    "process_eval.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ae371",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus\n",
    "\n",
    "- Mixed-precision\n",
    "- Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fbfef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixed-precision\n",
    "\n",
    "In many cases, you may want to **speed-up your training** by relying on mixed-precision operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daa61d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Autocast\n",
    "with th.cuda.amp.autocast():\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_op(outputs, targets)\n",
    "    \n",
    "# GradScaler\n",
    "scaler = th.cuda.amp.GradScaler()\n",
    "\n",
    "loss = ...\n",
    "optimizer = ...\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865318f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### th.cuda.amp.autocast()\n",
    "\n",
    "- Automatically **casts down** heavy operations (e.g., convolution, matrix multiplication) to 16-bit\n",
    "- Allows mixed-precision computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092cdf7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### th.cuda.amp.GradScaler()\n",
    "\n",
    "- Allows to work with 16-bit gradient values while **avoiding under/over-flows**\n",
    "- Scales up loss to **avoid underflows**\n",
    "- Scale gradient values down during gradient update **to ensure correct** model weights update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1dbb2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Tensorflow, mixed-precision is pretty easy to setup as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198ce6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1558391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient accumulation\n",
    "\n",
    "Gradient accumulation is the technique of accumulating gradients over **multiple batches** and perform a **single cumulative gradient update**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad4ce2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ Allows training with **bigger batch sizes** to allow for robust optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1830649",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tensorflow\n",
    "\n",
    "A quick way for doing gradient accumulation is to leverage the [```gradient-accumulator```](https://pypi.org/project/gradient-accumulator/) library\n",
    "\n",
    "$\\rightarrow$ since there is **no straightforward way** of doing gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6bc8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gradient_accumulator import GradientAccumulateModel\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model = Model(...)\n",
    "model = GradientAccumulateModel(accum_steps=4, \n",
    "                                inputs=model.input,\n",
    "                                outputs=model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9487da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_size = 4\n",
    "\n",
    "for step in range(steps):\n",
    "    batch_x, batch_y = next(train_iterator)\n",
    "    preds = model.batch_predict(x=batch_x)\n",
    "    loss = model.loss_op(y_pred=preds, y_true=batch_y)\n",
    "    \n",
    "    # Normalize loss to account for batch accumulation\n",
    "    loss = loss / accumulation_size\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if (step + 1) % accumulation_size == 0 or step == len(steps) - 1:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68c452",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "- Machine learning always starts from data! $\\rightarrow$ deep learning is no exception!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5bead",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The general recommendation is always to start small and progressively increase complexity (*I understand you want to try out your fancy architecture you saw in your dreams...*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b7b16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tensorflow and Pytorch mainly **share the same type of common mistakes and best practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be101",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 次回 (Jikai!)\n",
    "\n",
    "Actually, **there's nothing left** to show you... (or, better, there is still **way too much** stuff to talk about!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785418e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the first edition, 10 hours were not enough. This year edition is no exception!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bd9cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Anyway, I thought it could have been a **good opportunity** to show you something I've been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3256c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any questions?\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"../Images/Lecture-3/jojo-arrivederci.gif\" width=\"1200\" alt='JOJO_arrivederci'/>\n",
    "</div>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
